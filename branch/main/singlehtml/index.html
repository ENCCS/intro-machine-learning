

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Introduction to Machine Learning documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
      <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
      <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
      <link rel="stylesheet" type="text/css" href="_static/sphinx_lesson.css" />
      <link rel="stylesheet" type="text/css" href="_static/term_role_formatting.css" />
      <link rel="stylesheet" type="text/css" href="_static/sphinx_rtd_theme_ext_color_contrast.css" />
      <link rel="stylesheet" type="text/css" href="_static/overrides.css" />

  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js"></script>
      <script src="_static/doctools.js"></script>
      <script src="_static/sphinx_highlight.js"></script>
      <script src="_static/clipboard.min.js"></script>
      <script src="_static/copybutton.js"></script>
      <script src="_static/minipres.js"></script>
      <script>let toggleHintShow = 'Click to show';</script>
      <script>let toggleHintHide = 'Click to hide';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="_static/togglebutton.js"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="#" class="icon icon-home">
            Introduction to Machine Learning
              <img src="_static/ENCCS.jpg" class="logo" alt="Logo"/>
          </a>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">The lesson</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="#document-01-intro-to-ML">Introduction to Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-02-fundamentals-of-ML">Fundamentals of Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-03-scientific-data-for-ML">Scientific Data for Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-04-supervised-ML-classification">Supervised Learning (I): Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-05-supervised-ML-regression">Supervised Learning (II): Regression</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="#document-quick-reference">Quick Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="#document-guide">Instructor’s guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">About</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://enccs.se/lessons/">All lessons</a></li>
<li class="toctree-l1"><a class="reference external" href="https://enccs.se/">ENCCS</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">Introduction to Machine Learning</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Introduction to Machine Learning  documentation</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/ENCCS/intro-machine-learning/blob/main/content/index" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="introduction-to-machine-learning">
<h1>Introduction to Machine Learning<a class="headerlink" href="#introduction-to-machine-learning" title="Link to this heading"></a></h1>
<p>Machine Learning (ML) is a rapidly growing field within artificial intelligence (AI) that focuses on building systems capable of learning from data. Instead of being explicitly programmed with detailed rules, the ML models identify patterns and make predictions or decisions based on historical data. This approach has revolutionized many industries, including healthcare, finance, marketing, and technology, enabling applications like personalized recommendations, fraud detection, and speech recognition. As the volume of data continues to grow, understanding ML concepts and techniques becomes increasingly important for anyone interested in working with data or building intelligent systems.</p>
<p>This course provides a comprehensive introduction to the fundamental principles and techniques of ML. We will cover essential topics such as supervised learning, unsupervised learning, and model evaluation, as well as the basics of data preprocessing and feature selection. The participants will learn how to design and implement basic ML models using tools and frameworks commonly used in the industry. Through a combination of theory, practical exercises, and real-world examples, the participants will gain a solid foundation in ML, preparing them for further study or practical application in various domains.</p>
<p>By the end of the course, the participants will understand how the ML models are built, how to select the appropriate algorithms for specific problems, and how to assess model performance. In addition, the participants will be equipped with the skills to work on ML projects, from data collection and preparation to model training and evaluation. This knowledge will serve as a valuable stepping stone for those wishing to explore more advanced topics or specialize in areas such as deep learning (DL), natural language processing, or computer vision.</p>
<div class="admonition-prerequisites prerequisites admonition" id="prerequisites-0">
<p class="admonition-title">Prerequisites</p>
<ul class="simple">
<li><p>Familiarity with Python basics (loops, functions, lists, dictionaries) and libraries like NumPy, Pandas, and Matplotlib/Seaborn</p></li>
<li><p>Understanding vectors, matrices, matrix operations (multiplication, transpose, inverse), and eigenvalues</p></li>
<li><p>(Optional but useful): Derivatives, gradients, and optimization (especially for understanding gradient descent)</p></li>
</ul>
</div>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><p>20 min</p></td>
<td><p><span class="xref std std-doc">filename</span></p></td>
</tr>
</tbody>
</table>
<div class="toctree-wrapper compound">
<span id="document-01-intro-to-ML"></span><section id="introduction-to-machine-learning">
<h2>Introduction to Machine Learning<a class="headerlink" href="#introduction-to-machine-learning" title="Link to this heading"></a></h2>
<div class="admonition-questions questions admonition" id="questions-0">
<p class="admonition-title">Questions</p>
<ul class="simple">
<li><p>What is Machine Learning?</p></li>
<li><p>What is the relationship between AI, ML, and DL?</p></li>
<li><p>Why should we embrance ML at the current stage?</p></li>
</ul>
</div>
<div class="admonition-objectives objectives admonition" id="objectives-0">
<p class="admonition-title">Objectives</p>
<ul class="simple">
<li><p>Describe a general description of ML</p></li>
<li><p>Clarify the relationship between AI, ML, and DL</p></li>
<li><p>Get familiar with representative real-word applications of ML</p></li>
</ul>
</div>
<section id="what-is-machine-learning">
<h3>What is Machine Learning<a class="headerlink" href="#what-is-machine-learning" title="Link to this heading"></a></h3>
<p>Machine learning (ML) is a field of computer science that studies algorithms and techniques for automating solutions to complex problems that are hard to program using conventional programing methods.</p>
<p>In conventional programming, the programmer explicitly codes the logic (rules) to transform inputs (data) into outputs (answers), making it suitable for well-defined, rule-based tasks. In ML, the system learns the logic (rules) from data and answers, making it ideal for complex, pattern-based tasks where explicit rules are hard to define. The choice between them depends on the problem, data availability, and complexity.</p>
</section>
<section id="relation-with-artificial-intelligence-and-deep-learning">
<h3>Relation with Artificial Intelligence and Deep Learning<a class="headerlink" href="#relation-with-artificial-intelligence-and-deep-learning" title="Link to this heading"></a></h3>
<p>Artificial Intelligence (AI) is the broadest field, encompassing any technique that enables computers to mimic human intelligence, such as reasoning, problem-solving, perception, and decision-making. AI includes a wide range of approaches, from rule-based systems (like expert systems) to modern data-driven methods. It aims to create systems that can perform tasks that typically require human intelligence, such as playing chess, recognizing images, or understanding language.</p>
<p>ML is a subset of AI that focuses on algorithms and models that learn patterns from data to make predictions or decisions without being explicitly programmed. ML is one of the primary ways to achieve AI. It enables systems to improve performance over time by learning from experience (data) rather than relying solely on hardcoded rules. ML includes various techniques like supervised learning (<em>e.g.</em>, regression, classification), unsupervised learning (<em>e.g.</em>, clustering, dimensionality reduction), and reinforcement learning.</p>
<p>Deep Learning (DL) is a specialized subset of ML that uses neural networks with many layers (hence “deep”) to model complex patterns in large datasets. DL is a subset of ML, and it leverages artificial neural networks inspired by the human brain to tackle tasks like image recognition, speech processing, and natural language understanding. DL excels in handling unstructured data (<em>e.g.</em>, images, audio, text) and requires significant computational power and large datasets for training.</p>
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="_images/relationship-AI-ML-DL.png"><img alt="_images/relationship-AI-ML-DL.png" src="_images/relationship-AI-ML-DL.png" style="width: 512px;" />
</a>
<figcaption>
<p><span class="caption-text">The relationship between artificial intelligence, machine learning, and deep learning.</span><a class="headerlink" href="#id1" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="why-machine-learning">
<h3>Why Machine Learning?<a class="headerlink" href="#why-machine-learning" title="Link to this heading"></a></h3>
<p>ML is transforming how we solve complex problems in the real world by enabling systems to learn directly from data, rather than relying on explicitly programmed rules. In many real-world scenarios, such as medical diagnosis, stock market prediction, or natural language processing, the relationships between inputs and outputs are too complex or dynamic to define manually. ML models can uncover hidden patterns and make accurate predictions or decisions, making them essential tools in fields like healthcare, finance, transportation, and cybersecurity.</p>
<p>Another crucial advantage of ML is its ability to adapt and improve over time as more data becomes available. Unlike traditional rule-based systems that require constant manual updates, ML models can retrain and adjust themselves to new data, trends, or anomalies, ensuring that the system stays relevant and effective. For example, in fraud detection, ML algorithms can evolve as fraud tactics change, providing a stronger defense compared to static rules that may become outdated. This adaptability makes ML particularly powerful in dynamic, real-time environments where traditional programming methods fall short.</p>
<p>In addition, ML empowers the automation of complex tasks that were previously dependent on human expertise and intuition. From voice recognition in virtual assistants to autonomous driving, ML algorithms can process vast amounts of unstructured data such as text, images, and audio, which are traditionally challenging for computers to handle. By enabling machines to “learn” from experience and improve their performance over time, ML not only enhances productivity but also opens new frontiers for innovation across industries, creating smarter systems that can make meaningful contributions to society.</p>
</section>
<section id="machine-learning-applications">
<h3>Machine Learning Applications<a class="headerlink" href="#machine-learning-applications" title="Link to this heading"></a></h3>
<section id="problems-can-be-solve-with-ml">
<h4>Problems can be solve with ML<a class="headerlink" href="#problems-can-be-solve-with-ml" title="Link to this heading"></a></h4>
<p>ML is used across a wide range of industries and real-world problems in healthcare, finance, natural language processing, computer vision, transportation, manufacturing industry, retail, and cybersecurity.</p>
<p>Below are key categories of problems that can be applied using ML.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 16.7%" />
<col style="width: 83.3%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Application area</p></th>
<th class="head"><p>Example use Cases</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Healthcare</p></td>
<td><p>Disease prediction &amp; diagnosis, medical image analysis, drug discovery</p></td>
</tr>
<tr class="row-odd"><td><p>Finance</p></td>
<td><p>Fraud detection, credit scoring, algorithmic trading</p></td>
</tr>
<tr class="row-even"><td><p>Retail &amp; e-commerce</p></td>
<td><p>Product recommendations, customer segmentation, demand forecasting</p></td>
</tr>
<tr class="row-odd"><td><p>Transportation &amp; autonomous systems</p></td>
<td><p>Self-driving cars, traffic prediction, route optimization</p></td>
</tr>
<tr class="row-even"><td><p>Natural language processing (NLP)</p></td>
<td><p>Chatbots and virtual assistants, sentiment analysis, language translation</p></td>
</tr>
<tr class="row-odd"><td><p>Manufacturing &amp; industry</p></td>
<td><p>Predictive maintenance, quality control, supply chain optimization</p></td>
</tr>
<tr class="row-even"><td><p>Computer Vision</p></td>
<td><p>Facial recognition, object detection, image classification</p></td>
</tr>
</tbody>
</table>
</section>
<section id="problems-can-t-be-solve-with-ml">
<h4>Problems can’t be solve with ML<a class="headerlink" href="#problems-can-t-be-solve-with-ml" title="Link to this heading"></a></h4>
<p>ML is powerful, but it’s not magic. It’s a tool for finding patterns in data but has no idea what the patterns mean. Therefore it is not a substitute for human reasoning, creativity, or ethical judgment.</p>
<p>Below are key categories of problems that cannot be solved with ML due to inherent limitations, regardless of data or computational advancements.</p>
<ul class="simple">
<li><p>Problems with insufficient or poor-quality data: ML relies heavily on data. If data is scarce, noisy, biased, or unrepresentative, models fail to generalize. For example, predicting rare events with limited historical data (<em>e.g.</em>, catastrophic asteroid impacts, spread of pandemic) is unreliable.</p></li>
<li><p>Problems requiring reasoning, understanding, or deep logic. ML models approximate patterns but don’t understand them. They lack reasoning and common sense unless explicitly designed (<em>e.g.</em>, symbolic AI).</p></li>
<li><p>Problems that involve subjective judgments or value-based decisions. ML models don’t “know” what’s right or wrong – they reflect patterns in the data, including biases.</p></li>
<li><p>Problems outside of distribution generalization. A model trained on photos of cats can’t accurately classify dogs if it never saw dogs. ML models interpolate between known data. They struggle with novel scenarios far outside the training set.</p></li>
</ul>
</section>
<section id="problems-can-be-but-shouldn-t-be-solved-with-ml">
<h4>Problems can be, but shouldn’t be solved with ML<a class="headerlink" href="#problems-can-be-but-shouldn-t-be-solved-with-ml" title="Link to this heading"></a></h4>
<p>There are many problems where ML (or DL) could technically be applied, but shouldn’t be – either because of the simplicity of the problem or due to ethical, practical, or societal concerns.</p>
<ul class="simple">
<li><p>Tasks for modelling well defined systems, where the equations governing them are known and understood.</p></li>
<li><p>Problems at high-stakes domains with unacceptable error rates: ML can predict outcomes in fields like medical diagnosis or aviation safety, but even small errors can lead to catastrophic consequences. Over-reliance on ML without human oversight risks lives when models fail in edge cases.</p></li>
<li><p>Privacy-sensitive applications: ML can analyze personal data (<em>e.g.</em>, health records, browsing habits) to predict behaviors, but using it for invasive profiling, surveillance, or targeted manipulation (<em>e.g.</em>, hyper-personalized propaganda) raises serious privacy and autonomy concerns.</p></li>
<li><p>Reinforcing harmful social norms: ML can optimize systems like targeted advertising or content recommendation, but doing so can amplify harmful behaviors (<em>e.g.</em>, echo chambers, misinformation, or addiction to social media) if not carefully regulated.</p></li>
</ul>
</section>
</section>
</section>
<span id="document-02-fundamentals-of-ML"></span><section id="fundamentals-of-machine-learning">
<h2>Fundamentals of Machine Learning<a class="headerlink" href="#fundamentals-of-machine-learning" title="Link to this heading"></a></h2>
<div class="admonition-questions questions admonition" id="questions-0">
<p class="admonition-title">Questions</p>
<ul class="simple">
<li><p>what are the main types of MLs?</p></li>
</ul>
</div>
<div class="admonition-objectives objectives admonition" id="objectives-0">
<p class="admonition-title">Objectives</p>
<ul class="simple">
<li><p>Describe main types of ML</p></li>
</ul>
</div>
<section id="types-of-machine-learning">
<h3>Types of Machine Learning<a class="headerlink" href="#types-of-machine-learning" title="Link to this heading"></a></h3>
<p>ML can be broadly categorized into three main types depending on how the models learn from input data and the nature of the input data they process.</p>
<section id="supervised-learning">
<h4>Supervised learning<a class="headerlink" href="#supervised-learning" title="Link to this heading"></a></h4>
<p>In supervised learning, the model is trained on a labeled dataset, where each input is paired with a corresponding output (label). The goal is to learn a mapping from inputs to outputs to make predictions on new, unseen data.</p>
<p>Supervised learning has two subtypes: <strong>Classification</strong> (predicting discrete categories) and <strong>Regression</strong> (predicting continuous values).</p>
<p>Here are representative examples of these two subtypes in real-word problems:</p>
<ul class="simple">
<li><p><strong>Classification</strong>: email spam detection (spam/ham), image recognition (cat/dog), medical diagnosis (disease/no disease).</p></li>
<li><p><strong>Regression</strong>: house price prediction, weather forecasting.</p></li>
</ul>
</section>
<section id="unsupervised-learning">
<h4>Unsupervised learning<a class="headerlink" href="#unsupervised-learning" title="Link to this heading"></a></h4>
<p>In unsupervised learning, the model works with unlabeled data, identifying patterns, structures, or relationships within the data without explicit guidance on what to predict.</p>
<p>Unsupervised learning also has two subtypes: <strong>Clustering</strong> (grouping similar data points together) and <strong>Dimensionality reduction</strong> (simplifying data by reducing features while preserving important information)</p>
<p>Representative examples of these two subtypes in real-word problems:</p>
<ul class="simple">
<li><p><strong>Clustering</strong>: customer segmentation in marketing (grouping users by behavior), image segmentation (grouping similar pixels).</p></li>
<li><p><strong>Dimensionality reduction</strong>: compressing high-dimensional data (<em>e.g.</em>, reducing image features for faster processing), anomaly detection.</p></li>
</ul>
</section>
<section id="reinforcement-learning">
<h4>Reinforcement learning<a class="headerlink" href="#reinforcement-learning" title="Link to this heading"></a></h4>
<p>The model (agent) learns by interacting with an environment. It takes actions, receives feedback (rewards or penalties), and learns a strategy (policy) to maximize long-term rewards.</p>
<p>Representative examples of reinforcement learning in real-word problems: game-playing AI (<em>e.g.</em>, AlphaGo), robot navigation, autonomous driving.</p>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="_images/ML-three-types.png"><img alt="_images/ML-three-types.png" src="_images/ML-three-types.png" style="width: 512px;" />
</a>
<figcaption>
<p><span class="caption-text">Three main types of machine learning. Main approaches include classification and regression under the supervised learning and clustering under the unsupervised learning. Reinforcement learning enhance the model performance by interacting with environment. Coloured dots and triangles represent the training data. Yellow stars represent the new data which can be predicted by the trained model. This figure was taken from the paper <a class="reference external" href="https://www.frontiersin.org/journals/pharmacology/articles/10.3389/fphar.2021.720694/full">Machine Learning Techniques for Personalised Medicine Approaches in Immune-Mediated Chronic Inflammatory Diseases: Applications and Challenges</a>.</span><a class="headerlink" href="#id3" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="other-subtypes">
<h4>Other subtypes<a class="headerlink" href="#other-subtypes" title="Link to this heading"></a></h4>
<p>In addition to supervised and unsupervised learning, there are other important paradigms in machine learning.</p>
<ul class="simple">
<li><p><strong>Semi-supervised learning</strong> bridges the gap between supervised and unsupervised learning by using a small amount of labeled data together with a large amount of unlabeled data, helping models learn more effectively when labeling is expensive or time-consuming (<em>e.g.</em>, medical image analysis).</p></li>
<li><p><strong>Self-supervised learning</strong> is a form of unsupervised learning where the model generates its own labels from the data – typically for pretraining models on tasks like image or language understanding, enabling them to learn robust representations without explicit labels (<em>e.g.</em>, predicting the next word in a sentence, and filling in missing image patches)</p></li>
<li><p><strong>Transfer learning</strong> involves applying knowledge from a pretrained model, trained on a large, general dataset, to a new, related task, significantly reducing training time and data requirements (<em>e.g.</em>, fine-tuning a speech recognition model for a new dialect).</p></li>
</ul>
<p>These techniques expand the capabilities and versatility of machine learning across data-limited or computationally constrained environments.</p>
</section>
</section>
<section id="machine-learning-workflow">
<h3>Machine Learning Workflow<a class="headerlink" href="#machine-learning-workflow" title="Link to this heading"></a></h3>
<section id="what-is-a-workflow-for-ml">
<h4>What is a workflow for ML?<a class="headerlink" href="#what-is-a-workflow-for-ml" title="Link to this heading"></a></h4>
<p>A machine learning workflow is a structured approach for developing, training, evaluating, and deploying machine learning models. It typically involves several key phases, including data collection, preprocessing, model training and evaluation, and finally, deployment to production.</p>
<p>Here is a graphical representation of ML workflow, and a concise overview of the key steps are described below.</p>
</section>
<section id="problem-definition-and-project-setup">
<h4>Problem definition and project setup<a class="headerlink" href="#problem-definition-and-project-setup" title="Link to this heading"></a></h4>
<p><strong>Problem Definition</strong> is the first and most critical phase of any ML project. It sets the direction, scope, and goals for the entire project.</p>
<ul class="simple">
<li><p>we should understand the problem domain: what is the real-world problem we are trying to solve? are we predicting, classifying, or grouping data? (<em>e.g.</em>, predict house prices, detect spam emails, cluster customers)</p></li>
<li><p>we should determine if ML is the appropriate solution for the problem</p></li>
<li><p>we then should identify the expected outputs: what will the ML model produce? (<em>e.g.</em>, a number, a label, or a probability)</p></li>
<li><p>we define the type of ML task (<em>e.g.</em>, classification and regression tasks for supervised learning, clustering, dimensionality reduction for unsupervised learning, and decision-making tasks for reinforcement learning)</p></li>
</ul>
<p><strong>Project Setup</strong> is to set up the programming/development environment for the project.</p>
<ul class="simple">
<li><p>hardware requirements (CPU, SSD, GPU, cloud platforms, <em>etc.</em>)</p></li>
<li><p>software requirements (programming languages and libraries, ML/DL frameworks, and development tools, IDEs, Git/Docker, <a href="#id1"><span class="problematic" id="id2">*</span></a>etc.)</p></li>
<li><p>project structure: organize your project for clarity and scalability</p></li>
</ul>
<p>A typical ML project structure looks like this</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">ML_Project/</span>
<span class="go">├── data/                 # raw and processed data</span>
<span class="go">│   ├── raw/              # original, unprocessed data</span>
<span class="go">│   ├── processed/        # cleaned, preprocessed data</span>
<span class="go">├── notebooks/            # jupyter notebooks for EDA &amp; modeling</span>
<span class="go">├── src/                  # source code</span>
<span class="go">│   ├── utils/            # utility functions (*e.g.*, metrics, logging)</span>
<span class="go">│   ├── preprocessing.py  # data cleaning script</span>
<span class="go">│   └── train.py          # model training script</span>
<span class="go">├── models/               # trained model files (*e.g.*, .pkl, .h5)</span>
<span class="go">├── tests/                # unit and integration tests</span>
<span class="go">├── README.md             # project overview and setup instructions</span>
<span class="go">├── requirements.txt      # project dependencies</span>
<span class="go">├── config.yaml           # configuration file for hyperparameters and paths</span>
</pre></div>
</div>
</section>
<section id="data-collection-and-preprocessing">
<h4>Data collection and preprocessing<a class="headerlink" href="#data-collection-and-preprocessing" title="Link to this heading"></a></h4>
<p>In ML, data collection and preprocessing are crucial steps that significantly affect the performance of a model. High-quality, well-processed data leads to better predictions, while poor data can result in unreliable models.</p>
<ul class="simple">
<li><p><strong>Data collection</strong>: Gather the necessary data from various sources (<em>e.g.</em>, databases, APIs (twitter, linkedin, <em>etc.</em>), or manual collection), and ensure that data is representative and sufficient for the problem.</p></li>
<li><p><strong>Data preprocessing</strong>: Clean and prepare data by handling missing values (drop, impute, or predict), removing duplicates or irrelevant data, fixing inconsistencies (<em>e.g.</em>, “USA” vs. “United States”), normalizing/scaling features, encoding categorical variables, and addressing outliers, and other data quality issues.</p></li>
<li><p><strong>Exploratory data analysis (EDA)</strong>: Analyze data to uncover distributions, correlations, patterns, anomalies, and insights using visualizations and statistical methods. This helps in feature selection and understanding data distribution.</p></li>
<li><p><strong>Feature engineering</strong>: Create or select relevant features to improve model performance. This may involve dimensionality reduction (<em>e.g.</em>, PCA (principal component analysis)) or creating new features based on domain knowledge.</p></li>
<li><p><strong>Data splitting</strong>: Divide the dataset into training, validation, and test sets (<em>e.g.</em>, 70-15-15 split) to evaluate model performance and prevent overfitting.</p></li>
</ul>
</section>
<section id="model-selection-and-training">
<h4>Model selection and training<a class="headerlink" href="#model-selection-and-training" title="Link to this heading"></a></h4>
<p>Model Selection and Training refer to the process of choosing an appropriate model architecture and training it to learn patterns from data to solve a specific task. It involves selecting the appropriate algorithms (<em>e.g.</em>, linear/logistic regression, decision trees, neural networks, Gradient Boosting) based on the problem type, configuring its hyperparameters, and optimizing its parameters using training data to minimize error or maximize performance metrics.</p>
</section>
<section id="model-evaluation-and-assessment">
<h4>Model evaluation and assessment<a class="headerlink" href="#model-evaluation-and-assessment" title="Link to this heading"></a></h4>
<p>Model evaluation and assessment in machine learning refers to the process of measuring and analyzing a model’s performance to determine its effectiveness in solving a specific task. It involves using metrics and techniques to quantify how well the model generalizes to unseen data, identifies patterns, and meets desired objectives, typically using a test dataset separate from the training data.</p>
<p>Below are common evaluation metrics by task types:</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 40.0%" />
<col style="width: 60.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Task types</p></th>
<th class="head"><p>Evaluation metrics</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Classification</p></td>
<td><p>Accuracy, precision, recall, F1-score, ROC-AUC, <em>etc.</em></p></td>
</tr>
<tr class="row-odd"><td><p>Regression</p></td>
<td><p>Mean Squared Error (MSE), Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), R-squared, <em>etc.</em></p></td>
</tr>
<tr class="row-even"><td><p>Clustering</p></td>
<td><p>Silhouette score, Davies-Bouldin index, Calinski-Harabasz index.</p></td>
</tr>
<tr class="row-odd"><td><p>Ranking</p></td>
<td><p>Mean Reciprocal Rank (MRR), Normalized Discounted Cumulative Gain (NDCG).</p></td>
</tr>
<tr class="row-even"><td><p>NLP or generative tasks</p></td>
<td><p>BLEU, ROUGE, perplexity (often overlaps with deep learning).</p></td>
</tr>
</tbody>
</table>
<p>Here are representative techniques and processes for the assessment:</p>
<ul class="simple">
<li><p><strong>Train-validation-test split</strong>: Divide data into training (model learning), validation (hyperparameter tuning), and test (final evaluation) sets to prevent overfitting.</p></li>
<li><p><strong>Cross-validation</strong>: Use k-fold cross-validation to assess model stability across multiple data subsets.</p></li>
<li><p><strong>Confusion matrix</strong>: For classification, visualize true positives, false negatives, etc.</p></li>
<li><p><strong>Learning curves</strong>: Plot training <em>vs.</em> validation performance to diagnose underfitting or overfitting.</p></li>
<li><p><strong>Comparison with baselines</strong>: Comparing model performance against simple baselines (<em>e.g.</em>, random guessing, linear models) to ensure meaningful improvement.</p></li>
<li><p><strong>Robustness testing</strong>: Evaluate performance under noisy, adversarial, or out-of-distribution data.</p></li>
<li><p><strong>Fairness and bias analysis</strong>: Assess model predictions for fairness across groups (<em>e.g.</em>, demographics).</p></li>
</ul>
</section>
<section id="hyperparameter-tuning">
<h4>Hyperparameter tuning<a class="headerlink" href="#hyperparameter-tuning" title="Link to this heading"></a></h4>
<p>Hyperparameter tuning is the process of optimizing the settings (hyperparameters) of a model that are not learned during training but significantly affect its performance. These include parameters like learning rate, number of hidden layers, or batch size, which control the model’s behavior and training process.</p>
<p>The goal of this process is to find the best combination of hyperparameters that maximizes performance metrics (<em>e.g.</em>, accuracy, precision) on a validation set.</p>
</section>
<section id="model-deployment-monitoring-and-improvement">
<h4>Model deployment, monitoring, and improvement<a class="headerlink" href="#model-deployment-monitoring-and-improvement" title="Link to this heading"></a></h4>
<p>Model deployment, monitoring, and improvement refer to the processes involved in taking a trained machine learning model from development to production, ensuring it performs effectively in real-world applications, and continuously enhancing its performance.</p>
<ul class="simple">
<li><p><strong>Model deployment</strong> indicates an integration of a trained model into a production environment (APIs or cloud platforms) where it can make predictions or decisions on new, unseen data.</p></li>
<li><p>Once deployed, the model’s performance must be continuously tracked to ensure it remains accurate and reliable over time, which is termed as <strong>model monitoring</strong>.</p></li>
<li><p>As the models degrade over time, so continuous improvement is necessary. <strong>Model improvement</strong> involves updating or retraining the model to maintain or enhance its performance based on monitoring insights or new data.</p></li>
</ul>
</section>
</section>
<section id="machine-learning-libraries">
<h3>Machine Learning Libraries<a class="headerlink" href="#machine-learning-libraries" title="Link to this heading"></a></h3>
<section id="scikit-learn">
<h4>Scikit-learn<a class="headerlink" href="#scikit-learn" title="Link to this heading"></a></h4>
<p><strong>Scikit-learn</strong> is a widely-used, open-source Python library designed for <strong>classical machine learning</strong>, offering a variety of algorithms and tools for for tasks, such classification, regression, clustering, and dimensionality reduction. It supports supervised learning (<em>e.g.</em>, SVM (support vector machine), decision trees, random forests), unsupervised learning (<em>e.g.</em>, k-means, PCA (principal component analysis)), and semi-supervised learning, with robust tools for data preprocessing, model evaluation, and hyperparameter tuning via <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code>. Built on NumPy, SciPy, and Matplotlib, it is designed for ease of use, making it ideal for beginners and rapid prototyping. Scikit-Learn excels in handling small to medium-sized datasets and includes utilities for data preprocessing, model evaluation, hyperparameter tuning, and pipeline construction. However, it lacks support for DL and GPU acceleration, limiting its scalability for large datasets or complex neural network tasks.</p>
</section>
<section id="keras">
<h4>Keras<a class="headerlink" href="#keras" title="Link to this heading"></a></h4>
<p><strong>Keras</strong> is a high-level neural networks API that simplifies the process of building and training DL models. Originally an independent library, Keras is now tightly integrated with TensorFlow as its official high-level interface (but also usable standalone), offering an accessible way to experiment with DL without sacrificing performance. Keras provides user-friendly abstractions for layers, models, loss functions, and optimizers, allowing users for quick prototyping of neural networks for tasks like image classification, text generation, and time series forecasting with minimal code. Keras abstracts away much of the complexity of TensorFlow while retaining flexibility, making it ideal for beginners and those who need fast experimentation.</p>
</section>
<section id="tensorflow">
<h4>TensorFlow<a class="headerlink" href="#tensorflow" title="Link to this heading"></a></h4>
<p>Developed by Google, <strong>TensorFlow</strong> is a powerful open-source library primarily for DL but versatile enough for a broad range of ML tasks. It provides a flexible ecosystem for building complex models, including neural networks for computer vision, natural language processing, and time series analysis. TensorFlow supports distributed computing across CPUs, GPUs, and TPUs, making it suitable for both research and production at scale. Its robust features, such as TensorBoard for visualization, TensorFlow Serving for model deployment, and TensorFlow Lite for mobile inference, make it a comprehensive framework for end-to-end machine learning development. TensorFlow’s high-level Keras API simplifies model building, while its low-level operations provide flexibility for advanced research. TensorFlow is well-suited for tasks like image recognition, natural language processing (NLP), and reinforcement learning, though its complexity can pose a steeper learning curve for beginners compared to alternatives like PyTorch.</p>
</section>
<section id="pytorch">
<h4>PyTorch<a class="headerlink" href="#pytorch" title="Link to this heading"></a></h4>
<p>Developed by Facebook’s AI Research Lab (FAIR), PyTorch is auser-friendly and open-source DL library that has gained significant popularity in academia and industry. Known for its intuitive design and “define-by-run” (eager execution) approach, PyTorch allows developers to build, train, and debug models in a flexible and interactive manner. Its strong support for GPU acceleration and extensive ecosystem-ranging from computer vision (TorchVision) to NLP (TorchText) and audio (TorchAudio) – make it an excellent choice for cutting-edge DL research and production. Popular in academia and increasingly in industry, PyTorch excels in rapid prototyping and experimentation but is less optimized for production deployment compared to TensorFlow. Its active community and support for GPU acceleration make it a favorite for cutting-edge ML and DL research.</p>
</section>
<section id="xgboost-lightgbm">
<h4>XGBoost &amp; LightGBM<a class="headerlink" href="#xgboost-lightgbm" title="Link to this heading"></a></h4>
<p><strong>XGBoost</strong> (Extreme Gradient Boosting) and <strong>LightGBM</strong> (Light Gradient Boosting Machine) are high-performance gradient boosting libraries that have become go-to solutions for structured data problems, such as tabular datasets. Both libraries implement optimized gradient boosting algorithms that deliver fast training speeds, high accuracy, and scalability to large datasets. XGBoost is known for its robustness and versatility, while LightGBM offers further speed and memory efficiency through histogram-based algorithms and leaf-wise growth strategies. These libraries have become essential tools for data scientists working with structured data, outperforming traditional models in many real-world scenarios.</p>
</section>
<section id="hugging-face-transformers">
<h4>Hugging Face Transformers<a class="headerlink" href="#hugging-face-transformers" title="Link to this heading"></a></h4>
<p><strong>Hugging Face Transformers</strong> is a cutting-edge library that provides access to state-of-the-art pre-trained models for NLP tasks and computer vision, including text classification, translation, summarization, and question answering. The library’s pre-trained models and tokenizers simplify NLP workflows by enabling rapid experimentation with large language models, and in addition, this library supports both TensorFlow and PyTorch backends, integrating with datasets via Hugging Face’s datasets library, and has a vibrant community contributing to its continuous development.</p>
</section>
<section id="fastai">
<h4>FastAI<a class="headerlink" href="#fastai" title="Link to this heading"></a></h4>
<p><strong>FastAI</strong> is a high-level DL library built on PyTorch, designed to make AI accessible to a wider audience by simplifying complex tasks. It provides high-level abstractions and best practices out-of-the-box, allowing users to train powerful models with minimal code and optimal defaults. FastAI is particularly well-known for its transfer learning capabilities, enabling quick adaptation of pre-trained models for tasks like image classification and text generation. With its focus on practical usage, education, and strong community support, FastAI is ideal for beginners and practitioners who want to quickly deploy models without deep theoretical expertise.</p>
</section>
<section id="jax">
<h4>JAX<a class="headerlink" href="#jax" title="Link to this heading"></a></h4>
<p>JAX, developed by Google, combines NumPy-like syntax with automatic differentiation and GPU/TPU acceleration, making it ideal for high-performance ML research. It enables composable function transformations (gradients, JIT compilation) and scales efficiently across hardware. While not as high-level as TensorFlow or PyTorch, JAX is favored for cutting-edge numerical computing, physics simulations, and advanced neural network research where speed and flexibility are crucial.</p>
<p>These libraries cater to different needs: Scikit-learn for classical ML, TensorFlow and PyTorch for DL and scalability, Keras for simplicity, XGBoost for high-performance tabular data tasks, and Hugging Face for transformer-based applications. The choice of these libraries depends on the task, data type, scalability needs, user expertise, and whether the focus is research, prototyping, or production deployment.</p>
<p>A summary of best features and key strengths of these libraries are summarized below.</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 20.0%" />
<col style="width: 40.0%" />
<col style="width: 40.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Library</p></th>
<th class="head"><p>Best Feature</p></th>
<th class="head"><p>Key Strength</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Scikit-Learn</p></td>
<td><p>Simple and consistent API for classical ML tasks (classification, regression, clustering) and small/medium datasets.</p></td>
<td><p>Seamless integration with NumPy/Pandas and extensive documentation for ease-of-use with wide algorithm support</p></td>
</tr>
<tr class="row-odd"><td><p>PyTorch</p></td>
<td><p>Dynamic computation graph (define-by-run) for flexible model building and debugging</p></td>
<td><p>Flexible, intuitive framework with strong adoption for academic research in DL tasks</p></td>
</tr>
<tr class="row-even"><td><p>TensorFlow</p></td>
<td><p>Scalability with GPU/TPU acceleration for complex deep learning models</p></td>
<td><p>Excellent ecosystem (Keras, TF Hub, TF-Agents) for production-scale applications</p></td>
</tr>
<tr class="row-odd"><td><p>Keras</p></td>
<td><p>High-level, user-friendly API for rapid prototyping</p></td>
<td><p>Simplifies construction of DL models, making it beginner-friendly and efficient with TensorFlow compatibility for quick model development</p></td>
</tr>
<tr class="row-even"><td><p>XGBoost &amp; LightGBM</p></td>
<td><p>Optimized gradient boosting algorithms</p></td>
<td><p>Extremely effective for high-performance supervised learning with tabular/structured data</p></td>
</tr>
<tr class="row-odd"><td><p>Hugging Face Transformers</p></td>
<td><p>Extensive pretrained transformer models for easy fine-tuning</p></td>
<td><p>Community-driven ecosystem with user-friendly pipelines for NLP and vision tasks</p></td>
</tr>
<tr class="row-even"><td><p>FastAI</p></td>
<td><p>Transfer learning made easy for NLP &amp; vision tasks</p></td>
<td><p>Fast prototyping with minimal code and strong performance for applied deep learning</p></td>
</tr>
<tr class="row-odd"><td><p>JAX</p></td>
<td><p>NumPy + autodiff + GPU/TPU acceleration</p></td>
<td><p>Cutting-edge numerical computing, works with PyTorch/TensorFlow via interoperability libraries, but offers lower-level control</p></td>
</tr>
</tbody>
</table>
</section>
</section>
</section>
<span id="document-03-scientific-data-for-ML"></span><section id="scientific-data-for-machine-learning">
<h2>Scientific Data for Machine Learning<a class="headerlink" href="#scientific-data-for-machine-learning" title="Link to this heading"></a></h2>
<div class="admonition-questions questions admonition" id="questions-0">
<p class="admonition-title">Questions</p>
<ul class="simple">
<li><p>why data is super important in ML?</p></li>
<li><p>what is the type and form of scientific data?</p></li>
<li><p>what are the representative data storage formats?</p></li>
<li><p>which data structure is widely used in ML/DL applications?</p></li>
<li><p>what is data processing and feature engineering in ML/DL applications?</p></li>
</ul>
</div>
<section id="data-is-the-foundation-of-machine-learning">
<h3>Data is the Foundation of Machine Learning<a class="headerlink" href="#data-is-the-foundation-of-machine-learning" title="Link to this heading"></a></h3>
<p>Data is the backbone of ML as it serves as the foundation for training models to recognize patterns, make predictions, and generate insights. Without high-quality, relevant, and well-structured data, ML algorithms cannot learn meaningful patterns or make accurate predictions – poor or insufficient data leads to biased, unreliable, or ineffective AI systems. From image recognition to natural language processing, every ML application depends on properly curated datasets for training, validation, and testing. In addition, data determines the applicability and scalability of ML solutions across domains, from scientific research to real-world applications.</p>
<p>Moreover, data preparation and processing consume a significant portion of the ML workflow, often more time than model development itself. Cleaning, transforming, and structuring raw data into a usable format ensures that algorithms can extract valuable insights efficiently. The choice of data formats, like CSV for simplicity or HDF5 for large-scale datasets, also impacts data storage, accessibility, and computational efficiency during model training and deployment.</p>
<p>In this episode , we will</p>
</section>
<section id="understanding-scientific-data">
<h3>Understanding Scientific Data<a class="headerlink" href="#understanding-scientific-data" title="Link to this heading"></a></h3>
<p>Scientific data refers to any form of data that is collected, observed, measured, or generated as part of scientific research or experimentation. This data is used to support scientific analysis, develop theories, and validate hypotheses. It can come from a wide range of sources, including experiments, simulations, observations, or surveys across various scientific fields.</p>
<p>In general, scientific data can be described ty two terms: types of data and forms of data. They are related but distinct – types describe the nature of the data, while forms describe the how the data is structured and formatted (and stored, which will be discussed below).</p>
<section id="types-of-scientific-data">
<h4>Types of scientific data<a class="headerlink" href="#types-of-scientific-data" title="Link to this heading"></a></h4>
<p>Types of scientific data refer to what the data represents. It focuses on the nature or category of the data content.</p>
<ul>
<li><p><strong>Bit and byte</strong>: The smallest unit of storage in a computer is a <strong>bit</strong>, which holds either a 0 or a 1. Typically, eight bits are grouped together to form a <strong>byte</strong>. A single byte (8 bits) can represent up to 256 distinct values. By organizing bytes in various ways, computers can interpret and store different types of data.</p></li>
<li><p><strong>Numerical data</strong>: Different numerical data types (<em>e.g.</em>, integer and floating-point numbers) require different binary representation. Using more bytes for each value increases the range or precision, but it consumes more memory.</p>
<blockquote>
<div><ul>
<li><p>For example, integers stored with 1 byte (8 bits) have a range from [-128, 127], while with 2 bytes (16 bits) the range becomes [-32768, 32767]. Integers are whole numbers and can be represented exactly given enough bytes.</p></li>
<li><p>In contrast, floating-point numbers (used for decimals) often suffer from representation errors, since most fractional values cannot be precisely expressed in binary. These errors can accumulate during arithmetic operations. Therefore, in scientific computing, numerical algorithms must be carefully designed to minimize error accumulation. To ensure stability, floating-point numbers are typically allocated 8 bytes (64 bits), keeping approximation errors small enough to avoid unreliable results.</p></li>
<li><p>In ML/DL, half, single, and double precision refer to different formats for representing floating-point numbers, typically using 16, 32, and 64 bits, respectively.</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>Single precision</strong> (32-bit) is commonly used as a balance between computational efficiency and numerical accuracy.</p></li>
<li><p><strong>Half precision</strong> (16-bit) offers faster computation and reduced memory usage, making it popular for training large models on GPUs, though it may suffer from lower numerical stability.</p></li>
<li><p><strong>Double precision</strong> (64-bit) provides higher accuracy but is slower and more memory-intensive, so it’s mainly used when high numerical precision is critical.</p></li>
<li><p>Many modern frameworks, like TensorFlow and PyTorch, support mixed precision training, combining half and single precision to optimize performance while maintaining stability.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</li>
<li><p><strong>Text data</strong>: When it comes to text data, the simplest character encoding is ASCII (American Standard Code for Information Interchange), which was the most widely used encoding until 2008 when UTF-8 took over. The original ASCII uses only 7 bits for representing each character and therefore can encode 128 specified characters. Later, it became common to use an 8-bit byte to store each character, resulting in extended ASCII with support for up to 256 characters. As computers became more powerful and the need for including more characters from other alphabets, UTF-8 became the most common encoding. UTF-8 uses a minimum of one byte and up to four bytes per character. This flexibility makes UTF-8 ideal for modern applications requiring global character support.</p></li>
<li><p><strong>Metadata</strong>: Metadata encompasses diverse information about data, including units, timestamps, identifiers, and other descriptive attributes. While most scientific data is either numerical or textual, the associated metadata is usually domain-specific, and different types of data may have different metadata conventions. In scientific applications, such as simulations and experimental results, metadata is typically integrated with the corresponding dataset to ensure proper interpretation and reproducibility.</p></li>
</ul>
</section>
<section id="forms-of-scientific-data">
<h4>Forms of scientific data<a class="headerlink" href="#forms-of-scientific-data" title="Link to this heading"></a></h4>
<p>Forms of scientific data refer to how the data is structured or formatted. It focuses on the presentation or shape of the data.</p>
<ul class="simple">
<li><p><strong>Tabular data structure</strong> (numerical arrays) is a collection of numbers arranged in a specific structure that one can perform mathematical operations on. Examples of numerical arrays are scalar (0D), row or column vector (1D), matrix (2D), and tensor (3D), <em>etc.</em></p></li>
<li><p><strong>Textual data structure</strong> is a format for storing and organizing text-based data. It represents unstructured or semi-structured information as sequences of characters (letters, numbers, symbols, punctuation) arranged in strings.</p></li>
<li><p><strong>Images, videos, and audio</strong> are forms of scientific data that represent information through visual and auditory formats. Images capture static visual information as pixel arrays, videos combine sequential frames to show temporal changes, and audio encodes sound signals as time-series data for analysis.</p></li>
<li><p><strong>Graphs and networks</strong> are forms of scientific data that represent relationships between entities as nodes and connections as edges. They are used to model complex systems such as social networks, molecular interactions, and ecological food webs, capturing the structure and connectivity of scientific phenomena.</p></li>
</ul>
</section>
</section>
<section id="data-storage-format">
<h3>Data Storage Format<a class="headerlink" href="#data-storage-format" title="Link to this heading"></a></h3>
<section id="representative-data-storage-format">
<h4>Representative data storage format<a class="headerlink" href="#representative-data-storage-format" title="Link to this heading"></a></h4>
<p>When it comes to data storage, there are many types of storage formats used in scientific computing and data analysis. There isn’t one data storage format that works in all cases, so choose a file format that best suits your data.</p>
<p>For tabular data, each column usually has a name and a specific data type while each row is a distinct sample which provides data according to each column (including missing values). The simplest way to save tabular data is using the so-called CSV (comma-separated values) file, which is human-readable and easily shareable. However, it is not the best format to use when working with big (numerical) data.</p>
<p>Gridded data is another very common data type in which numerical data is normally saved in a multi-dimensional grid (array). Common field-agnostic array formats include:</p>
<ul class="simple">
<li><p><strong>Hierarchical Data Format</strong> (HDF5) is a high performance storage format for storing large amounts of data in multiple datasets in a single file. It is especially popular in fields where you need to store big multidimensional arrays such as physical sciences.</p></li>
<li><p><strong>Network Common Data Form version 4</strong> (NetCDF4) is a data format built on top of HDF5, but exposes a simpler API with a more standardised structure. NetCDF4 is one of the most used formats for storing large data from big simulations in physical sciences.</p></li>
<li><p><strong>Zarr</strong> is a data storage format designed for efficiently storing large, multi-dimensional arrays in a way that supports scalability, chunking, compression, and cloud-readiness.</p></li>
<li><p>There are more file formats like <a class="reference external" href="https://arrow.apache.org/docs/python/feather.html">feather</a>, <a class="reference external" href="https://arrow.apache.org/docs/python/parquet.html">parquet</a>, <a class="reference external" href="https://docs.xarray.dev/en/stable/">xarray</a> and <a class="reference external" href="https://numpy.org/doc/stable/reference/routines.io.html">npy</a> to store arrow tables or data frames.</p></li>
</ul>
</section>
<section id="overview-of-data-storage-format">
<h4>Overview of data storage format<a class="headerlink" href="#overview-of-data-storage-format" title="Link to this heading"></a></h4>
<p>Below is an overview of common data formats (✅ for <em>good</em>, 🟨 for <em>ok/depends on a case</em>, and ❌ for <em>bad</em>) adapted from Aalto university’s <a class="reference external" href="https://aaltoscicomp.github.io/python-for-scicomp/work-with-data/#what-is-a-data-format">Python for scientific computing</a>.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><div class="line-block">
<div class="line">Name:</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">Human</div>
<div class="line">readable:</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">Space</div>
<div class="line">efficiency:</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">Arbitrary</div>
<div class="line">data:</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">Tidy</div>
<div class="line">data:</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">Array</div>
<div class="line">data:</div>
</div>
</th>
<th class="head"><div class="line-block">
<div class="line">Long term</div>
<div class="line">storage/sharing:</div>
</div>
</th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="xref std std-ref">Pickle</span></p></td>
<td><p>❌</p></td>
<td><p>🟨</p></td>
<td><p>✅</p></td>
<td><p>🟨</p></td>
<td><p>🟨</p></td>
<td><p>❌</p></td>
</tr>
<tr class="row-odd"><td><p><span class="xref std std-ref">CSV</span></p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>🟨</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-even"><td><p><span class="xref std std-ref">Feather</span></p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
</tr>
<tr class="row-odd"><td><p><span class="xref std std-ref">Parquet</span></p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>🟨</p></td>
<td><p>✅</p></td>
<td><p>🟨</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-even"><td><p><span class="xref std std-ref">npy</span></p></td>
<td><p>❌</p></td>
<td><p>🟨</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
</tr>
<tr class="row-odd"><td><p><span class="xref std std-ref">HDF5</span></p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-even"><td><p><span class="xref std std-ref">NetCDF4</span></p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-odd"><td><p><span class="xref std std-ref">JSON</span></p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
<td><p>🟨</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-even"><td><p><span class="xref std std-ref">Excel</span></p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>🟨</p></td>
<td><p>❌</p></td>
<td><p>🟨</p></td>
</tr>
<tr class="row-odd"><td><p><span class="xref std std-ref">Graph formats</span></p></td>
<td><p>🟨</p></td>
<td><p>🟨</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
<td><p>✅</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="data-structures-for-ml-dl">
<h3>Data Structures for ML/DL<a class="headerlink" href="#data-structures-for-ml-dl" title="Link to this heading"></a></h3>
<p>ML (and DL) models require numerical input, so we must collect adaquate numerical data before training.
For ML tasks, multimedia data like image, audio, or video formats should be converted into tabular data or numerical arrays that ML models can process.
This conversion enables models to extract meaningful features, such as pixel intensities, audio frequencies or motion patterns, for tasks like classification or prediction.</p>
<section id="numerical-array">
<h4>Numerical array<a class="headerlink" href="#numerical-array" title="Link to this heading"></a></h4>
<p>Numerical array is a collection of numbers arranged in a specific structure that one can perform mathematical operations on. Examples of numerical arrays are scalar (0D), row or column vector (1D), matrix (2D), and tensor (3D), <em>etc.</em></p>
<p>Python offers powerful libraries like NumPy, PyTorch, TensorFlow, and Dask (parallel Numpy) to work with numerical arrays (0D to <a href="#id1"><span class="problematic" id="id2">*</span></a>n*D).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># 0D (Scalar)</span>
<span class="n">scalar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># 1D (Vector)</span>
<span class="n">vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>

<span class="c1"># 2D (Matrix)</span>
<span class="n">matrix_2D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>

<span class="c1"># 3D (Matrix)</span>
<span class="n">matrix_3D</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">matrix_3D</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="tensor">
<h4>Tensor<a class="headerlink" href="#tensor" title="Link to this heading"></a></h4>
<p>In ML and DL, a tensor is a mathematical object used to represent and manipulate multidimensional data. It generalizes scalars, vectors, and matrices to higher dimensions, serving as the fundamental data structure in frameworks like TensorFlow and PyTorch.</p>
<p>Why to use tensors in ML/DL (advantages of Tensor)?</p>
<ul class="simple">
<li><p>Generalization of scalars/vectors/matrices: Tensors extend these concepts to any number of dimensions, which is essential for handling complex data like images (3D) and videos (4D+).</p></li>
<li><p>Consistency: Tensors unify data structures across ML/DL frameworks, simplifying model building, training, and deployment.</p></li>
<li><p>Efficient computation: Frameworks like TensorFlow and PyTorch optimize tensor operations for speed (using GPUs/TPUs).</p></li>
<li><p>Neural network representations: Input data (images, text) is converted to tensors.</p></li>
<li><p>Automatic differentiation: Tensors support gradient tracking, which is vital for backpropagation in neural networks.</p></li>
</ul>
<p><a href="#id3"><span class="problematic" id="id4">`HERE &lt;&gt;`_</span></a> we provide a tutorial about Tensor including</p>
<ul>
<li><p>Tensor creation</p></li>
<li><p>Tensor’s properties (<cite>shape</cite>, <cite>dtype</cite>, <cite>ndim</cite>)</p></li>
<li><p>Tensor operations</p>
<blockquote>
<div><ul class="simple">
<li><p>indexing, slicing, transposing</p></li>
<li><p>element-wise operations: addition, subtraction, <em>etc.</em></p></li>
<li><p>matrix multiplication(<cite>np.dot</cite>, <cite>torch.matmul</cite>)</p></li>
<li><p>reshaping, flattening, squeezing, unsqueezing</p></li>
<li><p>reduction operations: sum, mean, max along axes</p></li>
<li><p>broadcasting: Rules and examples</p></li>
</ul>
</div></blockquote>
</li>
<li><p>Tensors in DL frameworks</p>
<blockquote>
<div><ul class="simple">
<li><p>moving tensors between CPUs and GPUs (suppose that you can access to GPU cards)</p></li>
</ul>
</div></blockquote>
</li>
</ul>
</section>
</section>
<section id="data-preprocessing">
<h3>Data Preprocessing<a class="headerlink" href="#data-preprocessing" title="Link to this heading"></a></h3>
<p>With the huge amount of data at disposal, more and more researchers and industry professionals are finding ways to use this data for research and commercial benefits. However, most of the data available by default is too raw. It is important to preprocess it before it can be used to identify important patterns or can be used to train statistical models that can be used to make predictions.</p>
<p>Data preprocessing refers to steps taken to integrate, clean, transform, and organize raw data into a format that can be effectively used by ML algorithms. It’s one of the most critical steps in the ML workflow because high-quality data leads to better model performance.</p>
<p><a href="#id5"><span class="problematic" id="id6">`HERE &lt;&gt;`_</span></a> we provide a tutorial addressing representative steps for preprocessing data in <a class="reference external" href="https://inria.github.io/scikit-learn-mooc/python_scripts/trees_dataset.html">penguins datasets</a> using Python libraries like <cite>numpy</cite>, <cite>pandas</cite>, <cite>matplotlib</cite>, <cite>seaborn</cite>, and <cite>scikit-learn</cite>.</p>
</section>
<section id="feature-engineering">
<h3>Feature Engineering<a class="headerlink" href="#feature-engineering" title="Link to this heading"></a></h3>
<p>Feature engineering is a part of the broader data processing pipeline in ML workflows. It involves using domain knowledge to select, modify, or create new features – variables or attributes – from existing data to help algorithms better understand patterns and relationships.</p>
<p>Feature engineering is crucial because the quality of features directly impacts a model’s predictive power. Well-crafted features can simplify complex patterns, reduce overfitting, and improve model interpretability, leading to better generalization and performance on unseen data. By tailoring features to the problem at hand, feature engineering bridges the gap between raw data and actionable insights, often making the difference between a mediocre and a high-performing model.</p>
<p>Feature engineering is closely related to data processing, but they serve different purposes.</p>
<ul class="simple">
<li><p>Data processing (or data preprocessing) is about cleaning and preparing data – handling missing values, removing duplicates, correcting data types, and ensuring consistency. This step makes the data <strong>usable</strong>.</p></li>
<li><p>Feature engineering, on the other hand, comes after basic processing and focuses on improving the predictive power of dataset.</p></li>
<li><p>In essence, <strong>data processing ensures data quality</strong>, while <strong>feature engineering enhances data value</strong> for ML models.</p></li>
<li><p>Both are essential steps in building effective and accurate predictive systems.</p></li>
</ul>
</section>
</section>
<span id="document-04-supervised-ML-classification"></span><section id="supervised-learning-i-classification">
<h2>Supervised Learning (I): Classification<a class="headerlink" href="#supervised-learning-i-classification" title="Link to this heading"></a></h2>
<div class="admonition-questions questions admonition" id="questions-0">
<p class="admonition-title">Questions</p>
<ul class="simple">
<li><p>why</p></li>
</ul>
</div>
<div class="admonition-objectives objectives admonition" id="objectives-0">
<p class="admonition-title">Objectives</p>
<ul class="simple">
<li><p>Explain</p></li>
</ul>
</div>
<div class="admonition-instructor-note instructor-note admonition" id="instructor-note-0">
<p class="admonition-title">Instructor note</p>
<ul class="simple">
<li><p>40 min teaching</p></li>
<li><p>40 min exercises</p></li>
</ul>
</div>
<p>Classification is a supervised ML task where the model predicts discrete class labels based on input features. It involves training a model on labeled data so that it can assign new data to predefined categories or classes based on patterns learned from labeled training data.</p>
<p>In binary classification, models predict one of two classes, such as spam or not spam for emails. Multiclass classification extends this to multiple categories, like classifying images as cats, dogs, or birds.</p>
<p>Common algorithms for classification task include k-Nearest Neighbors (KNN), Logistic Regression, Naive Bayes, Support Vector Machine (SVM), Decision Tree, Random Forest, Gradient Boosting, and Neural Networks.</p>
<p>In this episode we will perform supervised classification tasks to categorize penguins into three species – Adelie, Chinstrap, and Gentoo – based on their physical measurements (flipper length, body mass, <em>etc.</em>). We will build and train multiple classifier models as mentioned above. Each model will be evaluated using appropriate performance metrics like accuracy, precision, recall, and F1 score. By comparing the results across models, we aim to identify which classifier model provides the most accurate and reliable classification for this task.</p>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="_images/4-penguins-categories.png"><img alt="_images/4-penguins-categories.png" src="_images/4-penguins-categories.png" style="width: 640px;" />
</a>
<figcaption>
<p><span class="caption-text">The Palmer Penguins data were collected from 2007-2009 by Dr. Kristen Gorman with the <a class="reference external" href="https://lternet.edu/site/palmer-antarctica-lter/">Palmer Station Long Term Ecological Research Program</a>, part of the <a class="reference external" href="https://lternet.edu/">US Long Term Ecological Research Network</a>. The data were imported directly from the <a class="reference external" href="https://edirepository.org/">Environmental Data Initiative (EDI)</a> Data Portal, and are available for use by CC0 license (“No Rights Reserved”) in accordance with the <a class="reference external" href="https://lternet.edu/data-access-policy/">Palmer Station Data Policy</a>.</span><a class="headerlink" href="#id3" title="Link to this image"></a></p>
</figcaption>
</figure>
<section id="importing-dataset">
<h3>Importing Dataset<a class="headerlink" href="#importing-dataset" title="Link to this heading"></a></h3>
<p>Seaborn provides the Penguins dataset through its built-in data-loading functions. We can access it using <code class="docutils literal notranslate"><span class="pre">sns.load_dataset('penguin')</span></code> and then have a quick look at the data:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

<span class="n">penguins</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;penguins&#39;</span><span class="p">)</span>
<span class="n">penguins</span>
</pre></div>
</div>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td></td>
<td><p>species</p></td>
<td><p>island</p></td>
<td><p>bill_length_mm</p></td>
<td><p>bill_depth_mm</p></td>
<td><p>flipper_length_mm</p></td>
<td><p>body_mass_g</p></td>
<td><p>sex</p></td>
</tr>
<tr class="row-even"><td><p>0</p></td>
<td><p>Adelie</p></td>
<td><p>Torgersen</p></td>
<td><p>39.1</p></td>
<td><p>18.7</p></td>
<td><p>181.0</p></td>
<td><p>3750.0</p></td>
<td><p>Male</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>Adelie</p></td>
<td><p>Torgersen</p></td>
<td><p>39.5</p></td>
<td><p>17.4</p></td>
<td><p>186.0</p></td>
<td><p>3800.0</p></td>
<td><p>Female</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>Adelie</p></td>
<td><p>Torgersen</p></td>
<td><p>40.3</p></td>
<td><p>18.0</p></td>
<td><p>195.0</p></td>
<td><p>3250.0</p></td>
<td><p>Female</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>Adelie</p></td>
<td><p>Torgersen</p></td>
<td><p>NaN</p></td>
<td><p>NaN</p></td>
<td><p>NaN</p></td>
<td><p>NaN</p></td>
<td><p>NaN</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>Adelie</p></td>
<td><p>Torgersen</p></td>
<td><p>36.7</p></td>
<td><p>19.3</p></td>
<td><p>193.0</p></td>
<td><p>3450.0</p></td>
<td><p>Female</p></td>
</tr>
<tr class="row-odd"><td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
</tr>
<tr class="row-even"><td><p>339</p></td>
<td><p>Gentoo</p></td>
<td><p>Biscoe</p></td>
<td><p>NaN</p></td>
<td><p>NaN</p></td>
<td><p>NaN</p></td>
<td><p>NaN</p></td>
<td><p>NaN</p></td>
</tr>
<tr class="row-odd"><td><p>340</p></td>
<td><p>Gentoo</p></td>
<td><p>Biscoe</p></td>
<td><p>46.8</p></td>
<td><p>14.3</p></td>
<td><p>215.0</p></td>
<td><p>4850.0</p></td>
<td><p>Female</p></td>
</tr>
<tr class="row-even"><td><p>341</p></td>
<td><p>Gentoo</p></td>
<td><p>Biscoe</p></td>
<td><p>50.4</p></td>
<td><p>15.7</p></td>
<td><p>222.0</p></td>
<td><p>5750.0</p></td>
<td><p>Male</p></td>
</tr>
<tr class="row-odd"><td><p>342</p></td>
<td><p>Gentoo</p></td>
<td><p>Biscoe</p></td>
<td><p>45.2</p></td>
<td><p>14.8</p></td>
<td><p>212.0</p></td>
<td><p>5200.0</p></td>
<td><p>Female</p></td>
</tr>
<tr class="row-even"><td><p>343</p></td>
<td><p>Gentoo</p></td>
<td><p>Biscoe</p></td>
<td><p>49.9</p></td>
<td><p>16.1</p></td>
<td><p>213.0</p></td>
<td><p>5400.0</p></td>
<td><p>Male</p></td>
</tr>
</tbody>
</table>
<p>There are seven columns include:</p>
<ul class="simple">
<li><p><em>species</em>: penguin species (Adelie, Chinstrap, Gentoo)</p></li>
<li><p><em>island</em>: island where the penguin was found (Biscoe, Dream, Torgersen)</p></li>
<li><p><em>bill_length_mm</em>: length of the bill</p></li>
<li><p><em>bill_depth_mm</em>: depth of the bill</p></li>
<li><p><em>flipper_length_mm</em>: length of the flipper</p></li>
<li><p><em>body_mass_g</em>: body mass in grams</p></li>
<li><p><em>sex</em>: male or female</p></li>
</ul>
<p>Looking at numbers from <code class="docutils literal notranslate"><span class="pre">penguins</span></code> and <code class="docutils literal notranslate"><span class="pre">penguins.describe()</span></code> usually does not give a very good intuition about the data we are working with, we have the preference to visualize the data.</p>
<p>One nice visualization for datasets with relatively few attributes is the Pair Plot, which can be created using <code class="docutils literal notranslate"><span class="pre">sns.pairplot(...)</span></code>. It shows a scatterplot of each attribute plotted against each of the other attributes. By using the <code class="docutils literal notranslate"><span class="pre">hue='species'</span></code> setting for the pairplot the graphs on the diagonal are layered kernel density estimate plots for the different values of the <code class="docutils literal notranslate"><span class="pre">species</span></code> column.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">penguins_classification</span><span class="p">[[</span><span class="s2">&quot;species&quot;</span><span class="p">,</span> <span class="s2">&quot;bill_length_mm&quot;</span><span class="p">,</span> <span class="s2">&quot;bill_depth_mm&quot;</span><span class="p">,</span> <span class="s2">&quot;flipper_length_mm&quot;</span><span class="p">,</span> <span class="s2">&quot;body_mass_g&quot;</span><span class="p">]],</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;species&quot;</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/4-penguins-pairplot.png"><img alt="_images/4-penguins-pairplot.png" src="_images/4-penguins-pairplot.png" style="width: 640px;" />
</a>
</figure>
<div class="admonition-discussion exercise important admonition" id="exercise-0">
<p class="admonition-title">Discussion</p>
<p>Take a look at the pairplot we created. Consider the following questions:</p>
<ul class="simple">
<li><p>Is there any class that is easily distinguishable from the others?</p></li>
<li><p>Which combination of attributes shows the best separation for all 3 class labels at once?</p></li>
<li><p>(optional) Create a similar pairplot, but with <code class="docutils literal notranslate"><span class="pre">hue=&quot;sex&quot;</span></code>. Explain the patterns you see. Which combination of features distinguishes the two sexes best?</p></li>
</ul>
<div class="admonition-solution solution important dropdown admonition" id="solution-0">
<p class="admonition-title">Solution</p>
<ol class="arabic simple">
<li><p>The plots show that the green class (Gentoo) is somewhat more easily distinguishable from the other two.</p></li>
<li><p>Adelie and Chinstrap seem to be separable by a combination of bill length and bill depth (other combinations are also possible such as bill length and flipper length).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sns.pairplot(penguins_classification,</span> <span class="pre">hue=&quot;sex&quot;,</span> <span class="pre">height=2.0)</span></code>. From the plots you can see that for each species females have smaller bills and flippers, as well as a smaller body mass. You would need a combination of the species and the numerical features to successfully distinguish males from females. The combination of bill_depth_mm and body_mass_g gives the best separation.</p></li>
</ol>
</div>
</div>
</section>
<section id="data-processing">
<h3>Data Processing<a class="headerlink" href="#data-processing" title="Link to this heading"></a></h3>
<section id="handling-missing-values-and-outliers">
<h4>Handling missing values and outliers<a class="headerlink" href="#handling-missing-values-and-outliers" title="Link to this heading"></a></h4>
<p>For a ML task, the input data (features) and target data (label) are not yet in a right format to use. We need to pre-process the data (as what we did yesterday) to clean missing values using <code class="docutils literal notranslate"><span class="pre">penguins_classification</span> <span class="pre">=</span> <span class="pre">penguins.dropna()</span></code> and check duplicate values using <code class="docutils literal notranslate"><span class="pre">penguins_classification.duplicated().value_counts()</span></code>.</p>
<p>It is noted that we don’t have outliers in this dataset (as we have discussed this issue in the <a href="#id10"><span class="problematic" id="id11">`data processing &lt;&gt;`_</span></a> tutorial). For the other datasets you use for the first time, you should check if there are outliers for some features in the dataset, and then take steps to handle the outliers, either to imputate outliers with mean/median values or to remove abnormal outliers for simplicity.</p>
</section>
<section id="encoding-categorical-variables">
<h4>Encoding categorical variables<a class="headerlink" href="#encoding-categorical-variables" title="Link to this heading"></a></h4>
<p>In the classification task, we will use the categorical variable <em>species</em> as the label (target variable), and other columns as features to predict the species of penguins.</p>
<div class="admonition-discussion exercise important admonition" id="exercise-1">
<p class="admonition-title">Discussion</p>
<ul class="simple">
<li><p>why to use <em>species</em>?</p></li>
<li><p>why not to use other other categorical variables (here it would be <em>island</em> and <em>sex</em>)?</p></li>
</ul>
<div class="admonition-solution solution important dropdown admonition" id="solution-1">
<p class="admonition-title">Solution</p>
<ol class="arabic simple">
<li><p><em>species</em> will be the main biological classification target in this dataset as it 3 distinct classes (Adelie, Chinstrap, and Gentoo). This is commonly used in ML tutorials as a multi-class classification example (similar to the <a class="reference external" href="https://archive.ics.uci.edu/dataset/53/iris">Iris dataset</a>).</p></li>
<li><p><em>island</em> is not a ideal label as it is just geographical info, not a biological classification target; <em>sex</em> is possible but quite limited. This variable only has two classes (only for binary classification), and the data is unbalanced and has missing values.</p></li>
</ol>
</div>
</div>
<p>It is noted that ML models cannot directly process categorical (non-numeric) data, so we have to encode categorical variables like <em>species</em>, <em>island</em>, and <em>sex</em> into numerical values. Here we use <code class="docutils literal notranslate"><span class="pre">LabelEncoder</span></code> from <code class="docutils literal notranslate"><span class="pre">sklearn.preprocessing</span></code> to convert the species column, which serves as our classification target. The <code class="docutils literal notranslate"><span class="pre">LabelEncoder</span></code> assigns a unique integer to each species: “Adelie” becomes 0, “Chinstrap” becomes 1, and “Gentoo” becomes 2. This transformation allows classification algorithms to treat the species labels as distinct, unordered classes.</p>
<p>Then we apply the same rule to encode the island and sex columns. Although these are typically better handled with one-hot encoding due to their nominal nature, we use <code class="docutils literal notranslate"><span class="pre">LabelEncoder</span></code> here for simplicity and compact representation. Each unique category in island (<em>e.g.</em>, “Biscoe”, “Dream”, “Torgersen”) and sex (<em>e.g.</em>, “Male”, “Female”) is mapped to a unique integer. This enables us to include them as input features in the model without manual transformation. However, it’s important to note that <code class="docutils literal notranslate"><span class="pre">LabelEncoder</span></code> introduces an implicit ordinal relationship, which might not always be appropriate – in such cases, <code class="docutils literal notranslate"><span class="pre">OneHotEncoder</span></code> is preferred.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">LabelEncoder</span>

<span class="n">encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>

<span class="c1"># encode &quot;species&quot; column with 0=Adelie, 1=Chinstrap, and 2=Gentoo</span>
<span class="n">penguins_classification</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s1">&#39;species&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">penguins_classification</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">])</span>

<span class="c1"># encode &quot;island&quot; column with 0=Biscoe, 1=Dream and 2=Torgersen</span>
<span class="n">penguins_classification</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s1">&#39;island&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">penguins_classification</span><span class="p">[</span><span class="s1">&#39;island&#39;</span><span class="p">])</span>

<span class="c1"># encode &quot;sex&quot; with column 0=Female and 1=Male</span>
<span class="n">penguins_classification</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s1">&#39;sex&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">penguins_classification</span><span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">])</span>
</pre></div>
</div>
</section>
</section>
<section id="data-splitting">
<h3>Data Splitting<a class="headerlink" href="#data-splitting" title="Link to this heading"></a></h3>
<section id="splitting-features-and-labels">
<h4>Splitting features and labels<a class="headerlink" href="#splitting-features-and-labels" title="Link to this heading"></a></h4>
<p>In preparing the penguins dataset for classification, we first need to split the data into features and labels. The target variable we aim to predict is the penguin species, which we encode into numeric labels using <code class="docutils literal notranslate"><span class="pre">LabelEncoder</span></code>. This encoded species column will be the <strong>label vector</strong> (<em>e.g.</em>, <strong>y</strong>). The remaining columns – such as bill length, bill depth, flipper length, body mass, and encoded categorical variables like island and sex – constitute the <strong>feature matrix</strong> (<em>e.g.</em>, <strong>X</strong>). These features contain the input information the model will learn from.</p>
<p>Separating features (X) from labels (y) ensures a clear distinction between what the model uses for prediction and what it is trying to predict.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">penguins_classification</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;species&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">penguins_classification</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;int&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="splitting-training-and-testing-sets">
<h4>Splitting training and testing sets<a class="headerlink" href="#splitting-training-and-testing-sets" title="Link to this heading"></a></h4>
<p>After separating features and labels in the penguins dataset, we further divide the data into a training set and a testing set. The training set is used to train the model, allowing it to learn patterns and relationships from the data, and the test set, on the other hand, is reserved for evaluating the model’s performance on unseen data. A common split is 80% for training and 20% for testing, which provides enough data for training while still retaining a meaningful test set.</p>
<p>This splitting is typically done using the <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code> function from <code class="docutils literal notranslate"><span class="pre">sklearn.model_selection</span></code>, with a fixed <code class="docutils literal notranslate"><span class="pre">random_state</span></code> to ensure reproducibility.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of examples for training is </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span><span class="si">}</span><span class="s2"> and test is </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="feature-scaling">
<h4>Feature scaling<a class="headerlink" href="#feature-scaling" title="Link to this heading"></a></h4>
<p>Before training, it is also essential to ensure that numerical features are properly scaled via applying standardization or normalization – especially for distance-based or gradient-based models – to achieve optimal results.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># Standardize features</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>

<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="training-model-evaluating-model-performance">
<h3>Training Model &amp; Evaluating Model Performance<a class="headerlink" href="#training-model-evaluating-model-performance" title="Link to this heading"></a></h3>
<p>After preparing the Penguins dataset by handling missing values, encoding categorical variables, and splitting it into features-labels and training-test datasets, the next step is to apply classification algorithms including k-Nearest Neighbors (KNN), Naive Bayes, Decision Trees, Random Forests, and Neural Networks to predict penguin species based on their physical measurements. Each algorithm offers a unique approach to pattern recognition and generalization, and applying them to the same prepared dataset allows for a fair comparison of their predictive performance.</p>
<p>Below is the generic steps for representative algorithms we will use to training a model for penguins classification:</p>
<ul class="simple">
<li><p>choosing a model class and importing that model <code class="docutils literal notranslate"><span class="pre">from</span> <span class="pre">sklearn.neighbors</span> <span class="pre">import</span> <span class="pre">XXX</span></code></p></li>
<li><p>choosing the model hyperparameters by instantiating this class with desired values <code class="docutils literal notranslate"><span class="pre">xxx_model</span> <span class="pre">=</span> <span class="pre">XXX(&lt;...</span> <span class="pre">hyperparameters</span> <span class="pre">...&gt;)</span></code></p></li>
<li><p>training the model to the preprocessed train data by calling the <code class="docutils literal notranslate"><span class="pre">fit()</span></code> method of the model instance <code class="docutils literal notranslate"><span class="pre">xxx_model.fit(X_train_scaled,</span> <span class="pre">y_train)</span></code></p></li>
<li><p>making predictions using the trained model on test data <code class="docutils literal notranslate"><span class="pre">y_pred_xxx</span> <span class="pre">=</span> <span class="pre">xxx_model.predict(X_test_scaled)</span></code></p></li>
<li><p>evaluating model’s performance using available metrics <code class="docutils literal notranslate"><span class="pre">score_xxx</span> <span class="pre">=</span> <span class="pre">accuracy_score(y_test,</span> <span class="pre">y_pred_xxx)</span></code></p></li>
<li><p>(optional) data visualization of confusion matrix and relevant data</p></li>
</ul>
<section id="k-nearest-neighbors-knn">
<h4>k-Nearest Neighbors (KNN)<a class="headerlink" href="#k-nearest-neighbors-knn" title="Link to this heading"></a></h4>
<p>One intuitive and widely-used method is the KNN algorithm. KNN is a non-parametric, instance-based algorithm that predicts a sample’s label based on the majority class of its <em>k</em> closest neighbors in training set. <strong>KNN does not require training in the traditional sense; instead, it stores the entire dataset and performs computation during prediction time. This makes it a lazy learner but potentially expensive during inference.</strong></p>
<p>Here is an example of using the KNN algorithm to determine which class the new point belongs to. When the given query point, the KNN algorithm calculates the distance between this point and all points in the training dataset. It then selects the <em>k</em> points that are closest. The class with the most representatives among the <em>k</em> neighbors is chosen to be the prediction result for the query point. It is noted that the choice of <em>k</em> (the number of neighbors) significantly affects performance: a small <em>k</em> may be sensitive to noise, while a large <em>k</em> may smooth over important patterns.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/4-knn-example.png"><img alt="_images/4-knn-example.png" src="_images/4-knn-example.png" style="width: 640px;" />
</a>
</figure>
<p>Let’s create the KNN model. Here we choose 3 as the <em>k</em> value of the algorithm, which means that data needs 3 neighbors to be classified as one entity. Then we fit the train data using the <code class="docutils literal notranslate"><span class="pre">fit()</span></code> method.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.neighbors</span><span class="w"> </span><span class="kn">import</span> <span class="n">KNeighborsClassifier</span>

<span class="n">knn_model</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">knn_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
<p>After we fitting the training data, we use the trained model to predict species on the test set and evaluate its performance.</p>
<p>For classification tasks, metrics like accuracy, precision, recall, and the F1-score provide a comprehensive view of model performance.</p>
<ul class="simple">
<li><p><strong>accuracy</strong> measures the proportion of correctly classified instances across all species (Adelie, Chinstrap, Gentoo), and it gives an overall measure of how often the model is correct, but it can be misleading for imbalanced datasets.</p></li>
<li><p><strong>precision</strong> quantifies the proportion of correct positive predictions for each species, while <strong>recall</strong> assesses the proportion of actual positives correctly identified.</p></li>
<li><p>the <strong>F1-score</strong>, the harmonic mean of precision and recall, balances these metrics for each class, especially useful given the dataset’s imbalanced species distribution.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># predict on test data</span>
<span class="n">y_pred_knn</span> <span class="o">=</span> <span class="n">knn_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="c1"># evaluate model performance</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">accuracy_score</span>

<span class="n">score_knn</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_knn</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy for k-Nearest Neighbors:&quot;</span><span class="p">,</span> <span class="n">score_knn</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_knn</span><span class="p">))</span>
</pre></div>
</div>
<p>In classification tasks, a <strong>confusion matrix</strong> is a valuable tool for evaluating model performance by comparing predicted labels against true labels. For a multiclass classification task like the penguins dataset, the confusion matrix is an <strong>N x N</strong> matrix, where <strong>N</strong> is the number of target classes (here <strong>N=3</strong> for three penguins species). Each cell <em>(i, j)</em> in the matrix indicates the number of instances where the true class was <em>i</em> and the model predicted class <em>j</em>. Diagonal elements represent correct predictions, while off-diagonal elements indicate misclassifications. The confusion matrix provides an easy-to-understand overview of how often the predictions match the actual labels and where the model tends to make mistakes.</p>
<p>Since we will plot the confusion matrix multiple times, we write a function and call this function later whenever needed, which promotes clarity and avoids redundancy. This is especially helpful as we evaluate multiple classifiers such as KNN, Decision Trees, or SVM on the penguins dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">confusion_matrix</span>

<span class="k">def</span><span class="w"> </span><span class="nf">plot_confusion_matrix</span><span class="p">(</span><span class="n">conf_matrix</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">fig_name</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">conf_matrix</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;OrRd&#39;</span><span class="p">,</span>
                <span class="n">xticklabels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Adelie&quot;</span><span class="p">,</span> <span class="s2">&quot;Chinstrap&quot;</span><span class="p">,</span> <span class="s2">&quot;Gentoo&quot;</span><span class="p">],</span>
                <span class="n">yticklabels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Adelie&#39;</span><span class="p">,</span> <span class="s1">&#39;Chinstrap&#39;</span><span class="p">,</span> <span class="s1">&#39;Gentoo&#39;</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Predicted Label&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;True Label&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">fig_name</span><span class="p">)</span>
</pre></div>
</div>
<p>We compute the confusion matrix from the trined model using the KNN algorithm, and visualize the matrix.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cm_knn</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_knn</span><span class="p">)</span>

<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm_knn</span><span class="p">,</span> <span class="s2">&quot;Confusion Matrix using KNN algorithm&quot;</span><span class="p">,</span> <span class="s2">&quot;confusion-matrix-knn.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="_images/4-confusion-matrix-knn.png"><img alt="_images/4-confusion-matrix-knn.png" src="_images/4-confusion-matrix-knn.png" style="width: 420px;" />
</a>
<figcaption>
<p><span class="caption-text">The first row: there are 28 Adelie penguins in the test data, and all these penguins are identified as Adelie (valid). The second row: there are 20 Chinstrap pengunis in the test data, with 2 identified as Adelie (invalid), and 18 identified as Chinstrap (valid). The third row: there are 19 Gentoo penguins in the test data, and all these penguins are identified as Gentoo (valid).</span><a class="headerlink" href="#id4" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="logistic-regression">
<h4>Logistic Regression<a class="headerlink" href="#logistic-regression" title="Link to this heading"></a></h4>
<p><strong>Logistic Regression</strong> is a fundamental classification algorithm to predict categorical outcomes. Despite its name, logistic regression is not a regression algorithm but a classification method that predicts the probability of an instance belonging to a particular class.</p>
<p>For binary classification, it uses the logistic (<strong>sigmoid</strong>) function to map a linear combination of input features to a probability between 0 and 1, which is then thresholded (typically at 0.5) to assign a class.</p>
<p>For a multiclass classification, logistic regression can be extended using strategies like <strong>one-vs-rest</strong> (OvR) or softmax regression.</p>
<ul class="simple">
<li><p>in OvR, a separate binary classifier is trained for each species against all others.</p></li>
<li><p><strong>softmax regression</strong> generalizes the logistic function to compute probabilities across all classes simultaneously, selecting the class with the highest probability.</p></li>
</ul>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="_images/4-logistic-regression-example.png"><img alt="_images/4-logistic-regression-example.png" src="_images/4-logistic-regression-example.png" style="width: 640px;" />
</a>
<figcaption>
<p><span class="caption-text">(Upper left) the sigmoid function; (upper middle) the softmax regression process: three input features to the softmax regression model resulting in three output vectors where each contains the predicted probabilities for three possible classes; (upper right) a bar chart of softmax outputs in which each group of bars represents the predicted probability distribution over three classes; lower subplots) three binary classifiers distinguish one class from the other two classes using the one-vs-rest approach.</span><a class="headerlink" href="#id5" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>The creation of a Logistic Regression model and the process of fitting it to the training data are nearly identical to those used for the KNN model described above, except that a different classifier is selected. The code example and the resulting confusion matrix plot are provided below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="n">lr_model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span> <span class="o">=</span> <span class="mi">123</span><span class="p">)</span>
<span class="n">lr_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred_lr</span> <span class="o">=</span> <span class="n">lr_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="n">score_lr</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_lr</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy for Logistic Regression:&quot;</span><span class="p">,</span> <span class="n">score_lr</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_lr</span><span class="p">))</span>

<span class="n">cm_lr</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_lr</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm_lr</span><span class="p">,</span> <span class="s2">&quot;Confusion Matrix using Logistic Regression algorithm&quot;</span><span class="p">,</span> <span class="s2">&quot;confusion-matrix-lr.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/4-confusion-matrix-lr.png"><img alt="_images/4-confusion-matrix-lr.png" src="_images/4-confusion-matrix-lr.png" style="width: 420px;" />
</a>
</figure>
</section>
<section id="naive-bayes">
<h4>Naive Bayes<a class="headerlink" href="#naive-bayes" title="Link to this heading"></a></h4>
<p>The <strong>Naive Bayes</strong> algorithm is a simple yet powerful probabilistic classifier based on Bayes’ Theorem. This classifier assumes that all features are equally important and independent which is often not the case and may result in some bias. However, the assumption of independence simplifies the computations by turning conditional probabilities into products of probabilities. This algorithm computes the probability of each class given the input features and selects the class with the highest posterior probability.</p>
<p>Logistic regression and Naive Bayes are both popular algorithms for classification tasks, but they differ significantly in their approach, assumptions, and mechanics.</p>
<ul class="simple">
<li><p>Logistic regression is a <strong>discriminative</strong> model that directly models the probability of a data point belonging to a particular class by fitting a linear combination of features through a logistic (sigmoid) function for binary classification or softmax for multiclass tasks. For the penguins dataset, it would use features like bill length and flipper length to compute a weighted sum, transforming it into probabilities for species like Adelie, Chinstrap, or Gentoo. It assumes a linear relationship between features and the log-odds of the classes and optimizes parameters using maximum likelihood estimation, making it sensitive to feature scaling and correlations. Logistic regression is robust to noise and can handle correlated features to some extent, but it may struggle with highly non-linear relationships unless feature engineering is applied.</p></li>
<li><p>Naive Bayes, in contrast, is a <strong>generative</strong> model that relies on Bayes’ theorem to compute the probability of a class given the features, assuming conditional independence between features given the class. For the penguins dataset, it would estimate the likelihood of features (<em>e.g.</em>, bill depth) for each species and combine these with prior probabilities to predict the most likely species. The “naive” assumption of feature independence often doesn’t hold (<em>e.g.</em>, bill length and depth may be correlated), but Naive Bayes is computationally efficient, works well with high-dimensional data, and is less sensitive to irrelevant features. However, it can underperform when feature dependencies are significant or when the data distribution deviates from its assumptions (<em>e.g.</em>, Gaussian for continuous features in Gaussian Naive Bayes). Unlike logistic regression, it doesn’t require feature scaling but may need careful handling of zero probabilities (<em>e.g.</em>, via smoothing).</p></li>
</ul>
<p>Below is an example comparing Logistic Regression and Naive Bayes decision boundaries on a synthetic dataset having two features. The visualization highlights their fundamental differences in modeling assumptions and classification behavior: <strong>Logistic Regression learns a linear decision boundary directly, while Naive Bayes models feature distributions per class (assuming independence)</strong>.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/4-naive-bayes-example.png"><img alt="_images/4-naive-bayes-example.png" src="_images/4-naive-bayes-example.png" style="width: 640px;" />
</a>
</figure>
<p>To apply Naive Bayes, we use <code class="docutils literal notranslate"><span class="pre">GaussianNB</span></code> from <code class="docutils literal notranslate"><span class="pre">sklearn.naive_bayes</span></code>, which assumes that the features follow a Gaussian (normal) distribution, which is an appropriate choice for continuous numerical data such as bill length and body mass. Since Naive Bayes relies on probabilities, <strong>feature scaling is not required</strong>, but <strong>handling missing values and encoding categorical variables numerically is still necessary</strong>.</p>
<p>While Naive Bayes may not outperform more complex models like Random Forests, it offers <strong>fast training, low memory usage</strong>, and good performance for simple tasks.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.naive_bayes</span><span class="w"> </span><span class="kn">import</span> <span class="n">GaussianNB</span>

<span class="n">nb_model</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="n">nb_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred_nb</span> <span class="o">=</span> <span class="n">nb_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="n">score_nb</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_nb</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy for Naive Bayes:&quot;</span><span class="p">,</span> <span class="n">score_nb</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_nb</span><span class="p">))</span>

<span class="n">cm_nb</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_nb</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm_nb</span><span class="p">,</span> <span class="s2">&quot;Confusion Matrix using Naive Bayes algorithm&quot;</span><span class="p">,</span> <span class="s2">&quot;confusion-matrix-nb.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/4-confusion-matrix-nb.png"><img alt="_images/4-confusion-matrix-nb.png" src="_images/4-confusion-matrix-nb.png" style="width: 420px;" />
</a>
</figure>
</section>
<section id="support-vector-machine-svm">
<h4>Support Vector Machine (SVM)<a class="headerlink" href="#support-vector-machine-svm" title="Link to this heading"></a></h4>
<p>Previously we shown an example using Logistic Regression classifier producing a linear decision boundary that separates cats from dogs. It works by fitting a linear decision boundary that separates two classes based on the logistic function, making it particularly effective when the data is linearly separable. One characteristic of logistic regression is that the decision boundary tends to fall in the region where the probabilities of two classes are closest – typically where the model is most uncertain.</p>
<p>However, when there exists a large gap between two well-separated classes – as often occurs when distinguishing cats and dogs based on weight and ear length – logistic regression faces an inherent limitation: infinite possible solutions. The algorithm has no mechanism to select an “optimal” boundary when multiple valid linear separators exist in the wide margin between classes, and it will place the decision boundary somewhere in that gap, leading to a broad, undefined decision region with no supporting data. While this may not affect accuracy on clearly separated data, it can make the model less robust when new or noisy data appears near that boundary.</p>
<p>Below is an example, again, to separate cats from dogs based on ear length and weight. Besides the linear decision boundary from Logistic Regression classifier, we can find three additional linear boundaries that can also have a good separation of cats from dogs. Which one is better than the others and how to evaluate their performance on unseen data?</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/4-svm-example-large-gap.png"><img alt="_images/4-svm-example-large-gap.png" src="_images/4-svm-example-large-gap.png" style="width: 640px;" />
</a>
</figure>
<p>To better handle such situation, we can transition to the <strong>Support Vector Machine</strong> (SVM) algorithm. SVM takes a different approach by focusing on the concept of maximizing the margin – the distance between the decision boundary and the closest data points from each class (the support vectors) (as shown in the figure below). When there is a large gap between the two classes, SVM utilizes that space effectively by pushing the boundary toward the center of the gap while maintaining the maximum margin. This leads to a more stable and robust classifier, particularly in cases where the classes are well-separated.</p>
<p>Unlike Logistic Regression, which uses all data points to estimate probabilities, SVM relies primarily on the most critical examples (the ones nearest the boundary), making it less sensitive to outliers and more precise in defining class divisions.</p>
<figure class="align-center" id="id6">
<a class="reference internal image-reference" href="_images/4-svm-example-with-max-margin-separation.png"><img alt="_images/4-svm-example-with-max-margin-separation.png" src="_images/4-svm-example-with-max-margin-separation.png" style="width: 640px;" />
</a>
<figcaption>
<p><span class="caption-text">The SVM classification boundary for distinguishing cats and dogs based on ear length and weight. The solid black line represents the maximum margin hyperplane (decision boundary), while the dashed green lines show the positive and negative hyperplanes that define the margin. Black circles highlight the support vectors - the critical data points that determine the margin width.</span><a class="headerlink" href="#id6" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>To apply SVM, we use <code class="docutils literal notranslate"><span class="pre">SVC</span></code> (Support Vector Classification) from <code class="docutils literal notranslate"><span class="pre">sklearn.svm</span></code>, which by default assumes that the features follow a nonlinear relationship modeled by the <code class="docutils literal notranslate"><span class="pre">rbf</span></code> (Radial Basis Function) kernel. This kernel allows the model to find complex decision boundaries by implicitly mapping the input features into a higher-dimensional space. You can easily change the kernel to <code class="docutils literal notranslate"><span class="pre">linear</span></code>, <code class="docutils literal notranslate"><span class="pre">poly</span></code>, or <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> to experiment with different decision boundaries.</p>
<p>By adjusting the hyperparameters such as <code class="docutils literal notranslate"><span class="pre">C</span></code> (regularization strength) and <code class="docutils literal notranslate"><span class="pre">gamma</span></code> (kernel coefficient), we can control the trade-off between the margin width and classification accuracy. Below is a code example demonstrating how to use SVC with the RBF kernel for the penguins classification task.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.svm</span><span class="w"> </span><span class="kn">import</span> <span class="n">SVC</span>

<span class="n">svm_model</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">svm_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred_svm</span> <span class="o">=</span> <span class="n">svm_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="n">score_svm</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_svm</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy for Support Vector Machine:&quot;</span><span class="p">,</span> <span class="n">score_svm</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_svm</span><span class="p">))</span>

<span class="n">cm_svm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_svm</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm_svm</span><span class="p">,</span> <span class="s2">&quot;Confusion Matrix using Support Vector Machine algorithm&quot;</span><span class="p">,</span> <span class="s2">&quot;confusion-matrix-svm.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/4-confusion-matrix-svm.png"><img alt="_images/4-confusion-matrix-svm.png" src="_images/4-confusion-matrix-svm.png" style="width: 420px;" />
</a>
</figure>
</section>
<section id="decision-tree">
<h4>Decision Tree<a class="headerlink" href="#decision-tree" title="Link to this heading"></a></h4>
<p><strong>Decision Tree</strong> algorithm is a versatile and interpretable method for classification tasks. The core idea of this algorithm is to recursively split the dataset into smaller subsets based on feature thresholds creating a tree-like structure of decisions that result in the most significant separation of target classes.</p>
<p>Here is one example showing how to separate cats and dogs on the basis of two or three features.</p>
<figure class="align-center" id="id7">
<a class="reference internal image-reference" href="_images/4-decision-tree-example.png"><img alt="_images/4-decision-tree-example.png" src="_images/4-decision-tree-example.png" style="width: 640px;" />
</a>
<figcaption>
<p><span class="caption-text">(Upper) decision boundary separating cats and dogs based on two features (ear length and weight), and the corresponding decision tree structure; (lower): two decision boundaries separating cats and dogs based on three features (ear length, weight, and tail length), and the corresponding decision tree structure.</span><a class="headerlink" href="#id7" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>The code example for the Decision Tree classifier is provided below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.tree</span><span class="w"> </span><span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="n">dt_model</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">123</span><span class="p">)</span>
<span class="n">dt_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred_dt</span> <span class="o">=</span> <span class="n">dt_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="n">score_dt</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_dt</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy for Decision Tree:&quot;</span><span class="p">,</span> <span class="n">score_dt</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_dt</span><span class="p">))</span>

<span class="n">cm_dt</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_dt</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm_dt</span><span class="p">,</span> <span class="s2">&quot;Confusion Matrix using Decision Tree algorithm&quot;</span><span class="p">,</span> <span class="s2">&quot;confusion-matrix-dt.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/4-confusion-matrix-dt.png"><img alt="_images/4-confusion-matrix-dt.png" src="_images/4-confusion-matrix-dt.png" style="width: 420px;" />
</a>
</figure>
<p>We visualize the Decision Tree structure to understand how penguins are classified based on their physical characteristics.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.tree</span><span class="w"> </span><span class="kn">import</span> <span class="n">plot_tree</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plot_tree</span><span class="p">(</span><span class="n">dt_model</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">filled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rounded</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Decision Tree Structure for Penguins Species Classification&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/4-decision-tree-structure.png"><img alt="_images/4-decision-tree-structure.png" src="_images/4-decision-tree-structure.png" style="width: 640px;" />
</a>
</figure>
</section>
<section id="random-forest">
<h4>Random Forest<a class="headerlink" href="#random-forest" title="Link to this heading"></a></h4>
<p>While decision trees are easy to interpret and visualize, they come with some notable drawbacks. One of the primary issues is their tendency to overfit the training data, especially when the tree is allowed to grow deep without constraints like maximum depth or minimum samples per split. This leads to a model that captures noise in the training data, leading to poor generalization on unseen data, such as misclassifying a Gentoo penguin as Chinstrap due to overly specific splits. Additionally, decision trees are sensitive to small variations in the data – a slight change (<em>e.g.</em>, a few noisy measurements) in the dataset can result in a significantly different tree structure, reducing model stability and reliability.</p>
<p>To address these limitations, we can use an ensemble learning technique called <strong>Random Forest</strong>. A random forest builds upon the idea of decision trees by creating a large collection of them, each trained on a randomly selected subset of the data and features to produce a more accurate and stable prediction. By averaging the predictions of many trees (through majority voting for classification), random forest reduces overfitting, improves generalization, and mitigates the instability of individual trees.</p>
<p>Below is a figure demonstrating how Random Forest improves upon a single Decision Tree for classifying cats and dogs based on synthetic ear length and weight measurements.</p>
<figure class="align-center" id="id8">
<a class="reference internal image-reference" href="_images/4-random-forest-example.png"><img alt="_images/4-random-forest-example.png" src="_images/4-random-forest-example.png" style="width: 640px;" />
</a>
<figcaption>
<p><span class="caption-text">Top row shows the classification boundaries for both models. On the left, a single Decision Tree creates rigid, rectangular decision regions that precisely follow axis-aligned splits in the training data. While this achieves a good separation of the training samples, the jagged boundaries suggest potential overfitting to noise. In contrast, the Random Forest (right) produces smoother, more nuanced decision boundaries through majority voting across 100 trees. The blended purple transition zones represent areas where individual trees disagree, demonstrating how the ensemble averages out erratic predictions from any single tree. Bottom row reveals why Random Forests are more robust by examining three constituent trees. Tree #1 prioritizes ear length for its initial split, Tree #2 begins with weight, and Tree #3 uses a completely different weight threshold.</span><a class="headerlink" href="#id8" title="Link to this image"></a></p>
</figcaption>
</figure>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="n">rf_model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">rf_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred_rf</span> <span class="o">=</span> <span class="n">rf_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="n">score_rf</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_rf</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy for Random Forest:&quot;</span><span class="p">,</span> <span class="n">score_rf</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_rf</span><span class="p">))</span>

<span class="n">cm_rf</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_rf</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm_rf</span><span class="p">,</span> <span class="s2">&quot;Confusion Matrix using Random Forest algorithm&quot;</span><span class="p">,</span> <span class="s2">&quot;confusion-matrix-rf.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/4-confusion-matrix-rf.png"><img alt="_images/4-confusion-matrix-rf.png" src="_images/4-confusion-matrix-rf.png" style="width: 420px;" />
</a>
</figure>
<p>In addition to the confusion matrix, feature importance in a Random Forest (and also in Decision Tree) model provides valuable insight into which input features contribute most to the model’s predictions. Random Forest calculates feature importance by evaluating how much each feature decreases impurity – such as Gini impurity or entropy – when it is used to split the data across all decision trees in the forest. The higher the total impurity reduction attributed to a feature, the more important it is considered. These importance scores are then normalized to provide a relative ranking, helping identify which features are most influential in determining the output class. This information is especially useful for interpreting model behavior, selecting meaningful features, and understanding the underlying structure of the data.</p>
<p>Below is the code example for plotting the feature importance using a Random Forest algorithm to classify penguins into three categories.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">importances</span> <span class="o">=</span> <span class="n">rf_clf</span><span class="o">.</span><span class="n">feature_importances_</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">importances</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:orange&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Feature Importance&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Features&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Random Forest Feature Importance&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-center" id="id9">
<a class="reference internal image-reference" href="_images/4-random-forest-feature-importrance.png"><img alt="_images/4-random-forest-feature-importrance.png" src="_images/4-random-forest-feature-importrance.png" style="width: 512px;" />
</a>
<figcaption>
<p><span class="caption-text">Illustration of feature importance for penguin classification. Features with longer bars indicate greater influence in the classification decision, meaning the Random Forest relies more heavily on these measurements to correctly identify species.</span><a class="headerlink" href="#id9" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="gradient-boosting">
<h4>Gradient Boosting<a class="headerlink" href="#gradient-boosting" title="Link to this heading"></a></h4>
<p>We have trained the model using Decision Tree classifier, which offers an intuitive starting point for classifying penguin species based on their physical measurements (flipper length, body mass, <em>etc.</em>). This classifier is sensitive to small fluctuations in dataset, which often leads to overfitting, especially when the tree is deep.</p>
<p>To overcome the limitations of a single decision tree, we turned to Random Forest, which is an ensemble method that constructs multiple decision trees on different random subsets of the data and features. By averaging the predictions from each tree (in classification, taking a majority vote), random forests reduce overfitting and improve generalization. This approach balances model complexity with performance, and it offers a reliable estimate of feature importance, helping us understand which physical attributes are most influential in distinguishing penguin species.</p>
<p>While random forests offer robustness and improved accuracy over individual trees, we can push performance further by using <strong>Gradient Boosting</strong>. Gradient Boosting is also an ensemble learning technique that builds a strong classifier by combining many weak learners – typically shallow decision trees – in a sequential manner. Unlike Random Forest, which grows multiple trees independently and in parallel using random subsets of the data. Gradient Boosting constructs trees one at a time, where each new tree is trained to correct the errors made by its predecessors.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/4-random-forest-vs-gradient-boosting.png"><img alt="_images/4-random-forest-vs-gradient-boosting.png" src="_images/4-random-forest-vs-gradient-boosting.png" style="width: 512px;" />
</a>
</figure>
<p>In this code example below, we apply Gradient Boosting algorithm to classify penguin species. We use <code class="docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code> from scikit-learn due to its simplicity and strong baseline performance.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>

<span class="n">gb_model</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                                    <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">gb__model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred_gb</span> <span class="o">=</span> <span class="n">gb__model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="n">score_gb</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_gb</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy for Gradient Boosting:&quot;</span><span class="p">,</span> <span class="n">score_gb</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_gb</span><span class="p">))</span>

<span class="n">cm_gb</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_gb</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm_gb</span><span class="p">,</span> <span class="s2">&quot;Confusion Matrix using Gradient Boosting algorithm&quot;</span><span class="p">,</span> <span class="s2">&quot;confusion-matrix-gb.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/4-confusion-matrix-gb.png"><img alt="_images/4-confusion-matrix-gb.png" src="_images/4-confusion-matrix-gb.png" style="width: 420px;" />
</a>
</figure>
<p>This progression – from a single tree’s simplicity to random forests’ robustness and finally to gradient boosting’s precision – mirrors the evolution of <strong>tree-based methods</strong> in modern ML. While random forests remain excellent for baseline performance, Gradient Boosting often achieves state-of-the-art results for structured data like ecological measurements, provided careful tuning of the learning rate and tree depth.</p>
</section>
<section id="multi-layer-perceptron-scikit-learn">
<h4>Multi-Layer Perceptron (Scikit-Learn)<a class="headerlink" href="#multi-layer-perceptron-scikit-learn" title="Link to this heading"></a></h4>
<p>A <strong>Multilayer Perceptron</strong> (MLP) is a type of artificial neural network composed of multiple layers of interconnected perceptron, or neurons, that are designed to mimic the behavior of the human brain.</p>
<p>Each neuron (below figure)</p>
<ul>
<li><p>has one or more inputs (<cite>x_1</cite>, <cite>x_2</cite>, …), <em>e.g.</em>, input data expressed as floating point numbers</p></li>
<li><p>most of the time, each neuron conducts 3 main operations:</p>
<blockquote>
<div><ul class="simple">
<li><p>take the weighted sum of the inputs where (<cite>w_1</cite>, <cite>w_2</cite>, …) indicate weights</p></li>
<li><p>add an extra constant weight (<em>i.e.</em> a bias term) to this weighted sum</p></li>
<li><p>apply an activation function</p></li>
</ul>
</div></blockquote>
</li>
<li><p>return one output value</p></li>
<li><p>one example equation to calculate the output for a neuron is <cite>output = Activation(sum_i (x_i * w_i) + bias)</cite>.</p></li>
</ul>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/4-neuron-activation-function.png"><img alt="_images/4-neuron-activation-function.png" src="_images/4-neuron-activation-function.png" style="width: 512px;" />
</a>
</figure>
<p>An <strong>activation function</strong> is a mathematical transformation, and it converts the weighted sum of the inputs to the output signal of the neuron (perceptron). It introduces non-linearity to the network, enabling it to learn complex patterns and make decisions based on the weighted sum of inputs.</p>
<p>Below are representative activation functions commonly used in neural networks and DL models. Each function serves the crucial role of introducing non-linearities that enable neural networks to learn complex patterns and relationships in data.</p>
<ul class="simple">
<li><p>The <strong>sigmoid</strong> function, with its characteristic S-shaped curve, maps inputs to a smooth 0-1 range, making it historically popular for binary classification tasks.</p></li>
<li><p>The hyperbolic tangent (<strong>tanh</strong>) function, similar to sigmoid but ranging between -1 and 1, often demonstrates stronger gradients during training.</p></li>
<li><p>The <strong>Rectified Linear Unit</strong> (ReLU), which outputs zero for negative inputs and the identity for positive inputs, has become the default choice for many architectures due to its computational efficiency and effectiveness at mitigating the vanishing gradient problem.</p></li>
<li><p>The <strong>linear</strong> activation function (identity function) serves as an important reference point, demonstrating what network behavior would look like without any non-linear transformation.</p></li>
</ul>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/4-activation-function.png"><img alt="_images/4-activation-function.png" src="_images/4-activation-function.png" style="width: 640px;" />
</a>
</figure>
<p>A single neuron (perceptron), while capable of learning simple patterns, is limited in its ability to model complex relationships. By combining multiple neurons into layers and connecting them in a network, we create a powerful computational framework capable of approximating highly non-linear functions. In a MLP, neurons are organized into an input layer, one or more hidden layers, and an output layer.</p>
<p>The image below shows an example of a three-layer perceptron network having 3, 4, and 2 neurons in input, hidden and output layers.</p>
<ul class="simple">
<li><p>The input layer receives raw data, such as pixel values or measurements, and passes them to hidden layers.</p></li>
<li><p>The hidden layer contains multiple neurons that process the information and progressively extract higher-level features. Each neuron in a hidden layer is connected to neurons in adjacent layers, forming a dense web of weighted connections.</p></li>
<li><p>Finally, the output layer produces the network’s predictions, whether it’s a classification, regression output, or some other task.</p></li>
</ul>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/4-mlp-network.png"><img alt="_images/4-mlp-network.png" src="_images/4-mlp-network.png" style="width: 512px;" />
</a>
</figure>
<p>Here we build a three-layer perceptron for the penguins classification task using the <code class="docutils literal notranslate"><span class="pre">MLPClassifier</span></code> from <code class="docutils literal notranslate"><span class="pre">sklearn.neural_network</span></code>, which provides built-in functionality for training using backpropagation and gradient descent.</p>
<ul class="simple">
<li><p>this model is configured with an input layer matching the number of features (here we have four features for each penguin), a hidden layer with a specified number of neurons (<em>e.g.</em>, 16) to capture non-linear relationships, and an output layer with three nodes corresponding to penguins classes, using a <code class="docutils literal notranslate"><span class="pre">relu</span></code> activation function for the hidden layer neurons.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">adam</span></code> is the optimization algorithm used to update weight parameters</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">alpha</span></code> is the L2 regularization term (penalty). Setting this to 0 disables regularization, meaning the model won’t penalize large weights. This may lead to overfitting if the dataset is noisy or small.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">batch_size</span></code> is the number of samples per mini-batch during training. Smaller batch sizes lead to more frequent weight updates, which can result in more fine-grained learning but may increase noise and training time.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> specifies the learning rate schedule. “constant” means the learning rate remains fixed throughout training. Other options like “invscaling” or “adaptive” would change the learning rate during training.</p></li>
<li><p>With a constant learning rate, the <code class="docutils literal notranslate"><span class="pre">learning_rate_init=0.001</span></code> is used throughout training. A smaller value means slower learning, which may require more iterations but offers more stability.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_iter</span></code> specifies the maximum number of training iterations (epochs).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">random_state=123</span></code> controls the random number generation for weight initialization and data shuffling, ensuring reproducible results.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_iter_no_change=10</span></code> indicates that if validation score does not improve for 10 consecutive iterations, training will stop early. This is a form of early stopping to prevent overfitting or unnecessary computation.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.neural_network</span><span class="w"> </span><span class="kn">import</span> <span class="n">MLPClassifier</span>

<span class="n">mlp_model</span> <span class="o">=</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
                  <span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span>
                  <span class="n">learning_rate_init</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                  <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span> <span class="n">n_iter_no_change</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">mlp_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
<p>After fitting the model to the training data, we evaluate its accuracy on the test set, computing and then plotting the confusion matrix.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred_mlp</span> <span class="o">=</span> <span class="n">mlp_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="n">score_mlp</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_mlp</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy for MultiLayter Perceptron:&quot;</span><span class="p">,</span> <span class="n">score_mlp</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_mlp</span><span class="p">))</span>

<span class="n">cm_mlp</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_mlp</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm_mlp</span><span class="p">,</span> <span class="s2">&quot;Confusion Matrix using Multi-Layer Perceptron algorithm&quot;</span><span class="p">,</span> <span class="s2">&quot;confusion-matrix-mlp.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/4-confusion-matrix-mlp.png"><img alt="_images/4-confusion-matrix-mlp.png" src="_images/4-confusion-matrix-mlp.png" style="width: 420px;" />
</a>
</figure>
</section>
<section id="deep-neural-networks-keras">
<h4>Deep Neural Networks (Keras)<a class="headerlink" href="#deep-neural-networks-keras" title="Link to this heading"></a></h4>
<p>The MLP represents a foundational architecture in neural networks, consisting of an input layer, one or more hidden layers, and an output layer. While MLPs excel at learning complex patterns from tabular data, their shallow depth (typically 1-2 hidden layers) limits their ability to handle very high-dimensional or abstract data such as raw images, audio, or text.</p>
<p>To address these limitations, deep neural networks (DNNs) extend the MLP framework by incorporating multiple hidden layers. These additional layers allow the model to learn highly abstract features through deep hierarchical representations: early layers might capture basic features (like edges or shapes), while deeper layers recognize complex objects or semantic patterns. This depth enables DNNs to outperform traditional MLPs in complex tasks requiring high-level feature extraction, such as computer vision and natural language processing.</p>
<p>DNNs have specialized architectures designed to handle different types of data (<em>e.g.</em>, spatial, temporal, and sequential data) and tasks more effectively.</p>
<ul class="simple">
<li><p>a standard feedforward deep neural network consists of stacked fully connected layers</p></li>
<li><p><strong>convolutional neural networks</strong> (CNNs) are particularly well-suited for image data. They use convolutional layers to automatically extract local features like edges, textures, and shapes, significantly reducing the number of parameters and improving generalization on visual tasks.</p></li>
<li><p><strong>recurrent neural network</strong> (RNN) is designed for sequential data such as time series, speech, or natural language. RNNs include loops that allow information to persist across time steps, enabling the model to learn dependencies over sequences. More advanced versions, like Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs), address the limitations of basic RNNs by managing long-term dependencies more effectively.</p></li>
<li><p>In addition to CNNs and RNNs, the <strong>Transformer</strong> architecture has emerged as the state-of-the-art in many language and vision tasks. Transformers rely entirely on attention mechanisms rather than recurrence or convolutions, enabling them to model global relationships in data more efficiently. This flexibility has made them the foundation of powerful models like BERT, GPT, and Vision Transformers (ViTs). These specialized deep learning architectures illustrate how tailoring the network design to the structure of the data can lead to significant performance gains and more efficient learning.</p></li>
</ul>
<p>Here we use the Keras package to construct a small DNN and apply it to the penguins classification task, demonstrating how even a compact architecture can effectively distinguish between penguin species (Adelie, Chinstrap, and Gentoo).</p>
<p>Since Keras is part of the TensorFlow framework, we need to install TensorFlow if it hasn’t been installed already. In a Jupyter notebook, we can run the command <code class="docutils literal notranslate"><span class="pre">!pip</span> <span class="pre">install</span> <span class="pre">tensorflow</span></code>. After installation, it’s recommended to comment out the installation command and restart the kernel to ensure the environment is properly updated before running the rest of the notebook.</p>
<p>In this example, we do not use the categorical features “island” and “sex”, so we remove them from both the training and testing datasets. We then encode the target label “species” using the <code class="docutils literal notranslate"><span class="pre">pd.get_dummies</span></code> method. After that, we split the data into training and testing sets and standardize the feature values to ensure consistent scaling for model training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="kn">import</span> <span class="n">keras</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">penguins_classification</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;species&#39;</span><span class="p">,</span><span class="s1">&#39;island&#39;</span><span class="p">,</span> <span class="s1">&#39;sex&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">penguins_classification</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;int&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">penguins_classification</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Adelie&#39;</span><span class="p">,</span> <span class="s1">&#39;Chinstrap&#39;</span><span class="p">,</span> <span class="s1">&#39;Gentoo&#39;</span><span class="p">]</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of examples for training is </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span><span class="si">}</span><span class="s2"> and test is </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
<p>When building a neural network model with Keras, there are two common approaches: using the <code class="docutils literal notranslate"><span class="pre">Sequential()</span></code> API in a step-by-step manner, or defining all layers at once within the <code class="docutils literal notranslate"><span class="pre">Sequential()</span></code> constructor.</p>
<p>In the first approach, we start by creating an empty model using <code class="docutils literal notranslate"><span class="pre">keras.Sequential()</span></code>, which initializes a sequential container for stacking layers in a linear fashion. Then we define each layer separately using the <code class="docutils literal notranslate"><span class="pre">Dense</span></code> class, specifying the number of neurons and activation functions for each layer, and finally stack all layers to a trainable model using <code class="docutils literal notranslate"><span class="pre">keras.Model()</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.layers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Dropout</span>

<span class="n">dnn_model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>

<span class="n">input_layer</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],))</span> <span class="c1"># 4 input features</span>

<span class="n">hidden_layer1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">input_layer</span><span class="p">)</span>
<span class="n">hidden_layer1</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)(</span><span class="n">hidden_layer1</span><span class="p">)</span>

<span class="n">hidden_layer2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">hidden_layer1</span><span class="p">)</span>
<span class="c1">#hidden_layer2 = Dropout(0.0)(hidden_layer2)</span>

<span class="n">hidden_layer3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">hidden_layer2</span><span class="p">)</span>

<span class="n">output_layer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)(</span><span class="n">hidden_layer3</span><span class="p">)</span> <span class="c1"># 3 classes</span>

<span class="n">dnn_model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">input_layer</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">output_layer</span><span class="p">)</span>
</pre></div>
</div>
<p>Alternatively, we can streamline the process by defining all layers inside the <code class="docutils literal notranslate"><span class="pre">Sequential()</span></code> constructor. This approach creates the model and its architecture in a single, compact step, improving readability and reducing boilerplate code. It’s convenient for simple feedforward networks where the layer order is linear and straightforward.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dnn_model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
   <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],)),</span> <span class="c1"># input: 4 input features</span>

   <span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
   <span class="c1"># combine two lines together &quot;Dense(32, activation=&#39;relu&#39;, input_shape=(X_train_scaled.shape[1],)),&quot;</span>
   <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>

   <span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
   <span class="c1"># Dropout(0.0),</span>

   <span class="n">Dense</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>

   <span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)</span> <span class="c1"># output: 3 classes</span>
<span class="p">])</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">keras.layers.Dropout()</span></code> is a regularization layer in Keras used to reduce overfitting in neural networks by randomly setting a fraction of input units to zero during training. <code class="docutils literal notranslate"><span class="pre">Dropout(0.2)</span></code> means 20% of the outputs of a specific layer will be set to zero randomly.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/4-dnn-network-dropout.png"><img alt="_images/4-dnn-network-dropout.png" src="_images/4-dnn-network-dropout.png" style="width: 512px;" />
</a>
</figure>
<p>We use <code class="docutils literal notranslate"><span class="pre">dnn_model.summary()</span></code> to print a concise summary of a neural network’s architecture. It provides an overview of the model’s layers, their output shapes, and the number of trainable parameters, helping you debug and understand the network’s structure.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/4-dnn-summary.png"><img alt="_images/4-dnn-summary.png" src="_images/4-dnn-summary.png" style="width: 640px;" />
</a>
</figure>
<p>Now we have designed a DNN that, in theory, should be capable of learning to classify penguins. However, before training can begin, we must specify two critical components: (1) a loss function to quantify prediction errors, (2) an optimizer to adjust the model’s weights during training</p>
<ul class="simple">
<li><p>For the loss function, we select categorical cross-entropy for multi-class classification, as it penalizes incorrect probabilistic predictions. In Keras this is implemented in the <code class="docutils literal notranslate"><span class="pre">keras.losses.CategoricalCrossentropy</span></code> class. This loss function works well in combination with the <code class="docutils literal notranslate"><span class="pre">softmax</span></code> activation function we chose earlier. For more information on the available loss functions in Keras you can check the <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses">documentation</a>.</p></li>
<li><p>The optimizer determines how efficiently the model converges to a solution. Keras gives us plenty of choices all of which have their own pros and cons, but for now let us go with the widely used <code class="docutils literal notranslate"><span class="pre">Adam</span></code> (adaptive momentum estimation) optimizer. Adam has a number of parameters, but the default values work well for most problems, and therefore we use it with its default parameters.</p></li>
</ul>
<p>We use <code class="docutils literal notranslate"><span class="pre">model.compile()</span></code> to combine the determined loss function and optimier together, before starting the training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">keras.optimizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Adam</span>

<span class="n">dnn_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">CategoricalCrossentropy</span><span class="p">())</span>
</pre></div>
</div>
<p>We are now ready to train the DNN model. Here we only set a different number of <code class="docutils literal notranslate"><span class="pre">epochs</span></code>. One training epoch means that every sample in the training data has been shown to the neural network and used to update its parameters. During training, we set <code class="docutils literal notranslate"><span class="pre">batch_size=16</span></code> to balance memory efficiency and gradient stability, while <code class="docutils literal notranslate"><span class="pre">verbose=1</span></code> enables progress bars to monitor each epoch’s loss and metrics in real-time.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">history</span> <span class="o">=</span> <span class="n">dnn_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">fit</span></code> method returns a history object that has a history attribute with the training loss and potentially other metrics per training epoch. It can be very insightful to plot the training loss to see how the training progresses. Using seaborn we can do this as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">history</span><span class="o">.</span><span class="n">epoch</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;tab:orange&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training Loss&#39;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/4-dnn-loss.png"><img alt="_images/4-dnn-loss.png" src="_images/4-dnn-loss.png" style="width: 512px;" />
</a>
</figure>
<p>Finally we evaluate its accuracy on the test set, computing and then plotting the confusion matrix.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># predict class probabilities</span>
<span class="n">y_pred_dnn_probs</span> <span class="o">=</span> <span class="n">dnn_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="c1"># convert probabilities to class labels</span>
<span class="n">y_pred_dnn</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_pred_dnn_probs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">score_dnn</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred_dnn</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy for Deep Neutron Network:&quot;</span><span class="p">,</span> <span class="n">score_dnn</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred_dnn</span><span class="p">))</span>

<span class="n">cm_dnn</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred_dnn</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm_dnn</span><span class="p">,</span> <span class="s2">&quot;Confusion Matrix using DNN algorithm&quot;</span><span class="p">,</span> <span class="s2">&quot;confusion-matrix-dnn.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/4-confusion-matrix-dnn.png"><img alt="_images/4-confusion-matrix-dnn.png" src="_images/4-confusion-matrix-dnn.png" style="width: 420px;" />
</a>
</figure>
</section>
</section>
<section id="comparison-of-trained-models">
<h3>Comparison of Trained Models<a class="headerlink" href="#comparison-of-trained-models" title="Link to this heading"></a></h3>
<p>To evaluate the performance of different ML algorithms in classifying penguin species, we compared their accuracy scores and confusion matrices. The algorithms tested include instance-based algorithm (KNN), probability-based algorithms (Logistic Regression and Naive Bayes), SVM, tree-based methods (Decision Tree, Random Forest, and Gradient Boosting), and network-based models (Multi-Layer Perceptron and Deep Neural Networks). Each model was trained on the same training set and evaluated on a common test set, with consistent preprocessing applied across all methods.</p>
<p>For the current settings for the training:</p>
<ul class="simple">
<li><p>the Multi-Layer Perceptron algorithm achieved the highest accuracy, demonstrating their effectiveness in capturing complex patterns and feature interactions in the Penguins dataset;</p></li>
<li><p>the Naive Bayes algorithm showed slightly lower accuracy, likely due to its strong independence assumption between features, which doesn’t fully hold in the dataset;</p></li>
<li><p>the other algorithms provided moderate performance.</p></li>
</ul>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/4-scores-for-all-models.png"><img alt="_images/4-scores-for-all-models.png" src="_images/4-scores-for-all-models.png" style="width: 640px;" />
</a>
</figure>
<p>The confusion matrices provided consistent and deeper insights into how each model handled class-level predictions:</p>
<ul class="simple">
<li><p>the Multi-Layer Perceptron algorithm showed well-balanced performance across all three penguin species;</p></li>
<li><p>the Naive Bayes algorithm, in contrast, confused Adelie and Chinstrap penguins, which can be attributed to overlapping feature distributions between these species;</p></li>
<li><p>the other algorithms made limited number of misclassified instances (mainly between Adelie and Chinstrap).</p></li>
</ul>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/4-compare-confusion-matrices.png"><img alt="_images/4-compare-confusion-matrices.png" src="_images/4-compare-confusion-matrices.png" style="width: 640px;" />
</a>
</figure>
</section>
</section>
<span id="document-05-supervised-ML-regression"></span><section id="supervised-learning-ii-regression">
<h2>Supervised Learning (II): Regression<a class="headerlink" href="#supervised-learning-ii-regression" title="Link to this heading"></a></h2>
<div class="admonition-questions questions admonition" id="questions-0">
<p class="admonition-title">Questions</p>
<ul class="simple">
<li><p>why</p></li>
</ul>
</div>
<div class="admonition-objectives objectives admonition" id="objectives-0">
<p class="admonition-title">Objectives</p>
<ul class="simple">
<li><p>Explain</p></li>
</ul>
</div>
<div class="admonition-instructor-note instructor-note admonition" id="instructor-note-0">
<p class="admonition-title">Instructor note</p>
<ul class="simple">
<li><p>40 min teaching</p></li>
<li><p>40 min exercises</p></li>
</ul>
</div>
<p>Regression is a type of supervised ML task where the goal is to predict a continuous numerical value based on input features. Unlike classification, which assigns inputs to discrete categories, regression models output real-valued predictions.</p>
<p>While the penguins dataset is most commonly used for classification tasks, it can also be used for regression problems by selecting a continuous target variable. For example, we might be interested in predicting a penguin’s body mass based on its physical measurements like bill length, bill depth, flipper length, and other available features.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/4-penguins-pairplot.png"><img alt="_images/4-penguins-pairplot.png" src="_images/4-penguins-pairplot.png" style="width: 640px;" />
</a>
</figure>
<p>Depending on model construction procedures, in this episode we explore a variety of regression algorithms to predict penguin body mass based on flipper length. These models are chosen to represent different categories of ML approaches, from simple to more complex and flexible methods.</p>
<ul class="simple">
<li><p>We begin with KNN regression, which makes predictions based on the average of the closest training samples. It’s a non-parametric, instance-based model that captures local patterns in the data.</p></li>
<li><p>Next we apply linear models, such as standard Linear Regression and Regularized Regression, which assume a straight-line relationship between flipper length and body mass. These models are interpretable and efficient, making them a solid baseline for comparison.</p></li>
<li><p>To address possible non-linear trends in the data, we incorporate non-linear models like Polynomial Regression with higher-degree terms and and Support Vector Regression (SVR) with RBF kernels</p></li>
<li><p>Tree-based models, including decision trees, random forests, and gradient boosting, offer a robust alternative by recursively partitioning the feature space or building ensembles to improve accuracy and handle non-linearities effectively</p></li>
<li><p>Finally, we explore neural networks as a universal function approximator, capable of learning intricate relationships but requiring larger datasets and computational resources.</p></li>
</ul>
<p>Each model’s performance is rigorously assessed using cross-validated metrics (RMSE (root mean squared error), R²), and the corresponding predictive curve reveals how well they capture the biological allometry between flipper length and body mass.</p>
<p>This tiered approach – from simple models like linear regression to more complex ones such as random forests and neural networks – ensures that we balance interpretability with predictive power. By progressing through these levels of model complexity, we aim to identify the most suitable algorithm for accurately predicting penguin body mass from flipper length, while maintaining an understanding of how each model interprets the relationship between features and target.</p>
<section id="data-preparation">
<h3>Data Preparation<a class="headerlink" href="#data-preparation" title="Link to this heading"></a></h3>
<p>From the pairplot, we can see that the relationship between body mass and flipper length is visually strong, suggesting a clear positive correlation (figure below) between these two variables. Therefore, we use this pair for the regression task, as their strong relationship makes them suitable for modeling and predicting body mass based on flipper length. By modeling this relationship, we aim to estimate body mass based on flipper length, which can be valuable in ecological studies and predictive modeling involving penguin morphology.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/5-penguins-bodyMass-flipperLength.png"><img alt="_images/5-penguins-bodyMass-flipperLength.png" src="_images/5-penguins-bodyMass-flipperLength.png" style="width: 512px;" />
</a>
</figure>
<p>Following the procedures adopted in the previous episode, we begin by importing the Penguins dataset and performing data preprocessing, including handling missing values and outliers. For the regression task, categorical features are not required, so there is no need to encode them.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

<span class="n">penguins</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;penguins&#39;</span><span class="p">)</span>

<span class="c1"># remove missing values</span>
<span class="n">penguins_regression</span> <span class="o">=</span> <span class="n">penguins</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>

<span class="c1"># check duplicate values from dataset</span>
<span class="n">penguins_regression</span><span class="o">.</span><span class="n">duplicated</span><span class="p">()</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>

<span class="c1"># calculate lower and upper limit of outlier using IQR method</span>
<span class="n">IQR</span> <span class="o">=</span> <span class="n">penguins_regression</span><span class="p">[</span><span class="s2">&quot;body_mass_g&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="mf">0.75</span><span class="p">)</span> <span class="o">-</span> <span class="n">penguins_regression</span><span class="p">[</span><span class="s2">&quot;body_mass_g&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="mf">0.25</span><span class="p">)</span>
<span class="n">lower_limit</span> <span class="o">=</span> <span class="n">penguins_regression</span><span class="p">[</span><span class="s2">&quot;body_mass_g&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="mf">0.25</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mf">1.5</span> <span class="o">*</span> <span class="n">IQR</span><span class="p">)</span>
<span class="n">upper_limit</span> <span class="o">=</span> <span class="n">penguins_regression</span><span class="p">[</span><span class="s2">&quot;body_mass_g&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="mf">0.75</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.5</span> <span class="o">*</span> <span class="n">IQR</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Body Mass:      lower limt of IQR = </span><span class="si">{</span><span class="n">lower_limit</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> and upper limit of IQR = </span><span class="si">{</span><span class="n">upper_limit</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">IQR</span> <span class="o">=</span> <span class="n">penguins_regression</span><span class="p">[</span><span class="s2">&quot;flipper_length_mm&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="mf">0.75</span><span class="p">)</span> <span class="o">-</span> <span class="n">penguins_regression</span><span class="p">[</span><span class="s2">&quot;flipper_length_mm&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="mf">0.25</span><span class="p">)</span>
<span class="n">lower_limit</span> <span class="o">=</span> <span class="n">penguins_regression</span><span class="p">[</span><span class="s2">&quot;flipper_length_mm&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="mf">0.25</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mf">1.5</span> <span class="o">*</span> <span class="n">IQR</span><span class="p">)</span>
<span class="n">upper_limit</span> <span class="o">=</span> <span class="n">penguins_regression</span><span class="p">[</span><span class="s2">&quot;flipper_length_mm&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="mf">0.75</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.5</span> <span class="o">*</span> <span class="n">IQR</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Flipper Length: lower limt of IQR = </span><span class="si">{</span><span class="n">lower_limit</span><span class="si">:</span><span class="s2">7.2f</span><span class="si">}</span><span class="s2"> and upper limit of IQR = </span><span class="si">{</span><span class="n">upper_limit</span><span class="si">:</span><span class="s2">7.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Next, we separate the dataset into features (flipper length) and labels (body mass), and then split it into training and testing sets. This is followed by feature scaling to standardize the data before model training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">penguins_regression</span><span class="p">[[</span><span class="s2">&quot;flipper_length_mm&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">penguins_regression</span><span class="p">[</span><span class="s2">&quot;body_mass_g&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>

<span class="c1"># standardize features</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="training-model-evaluating-model-performance">
<h3>Training Model &amp; Evaluating Model Performance<a class="headerlink" href="#training-model-evaluating-model-performance" title="Link to this heading"></a></h3>
<section id="k-nearest-neighbors-knn">
<h4>k-Nearest Neighbors (KNN)<a class="headerlink" href="#k-nearest-neighbors-knn" title="Link to this heading"></a></h4>
<p>We begin by applying the KNN algorithm to the penguin regression task, with a code example provided below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.neighbors</span><span class="w"> </span><span class="kn">import</span> <span class="n">KNeighborsRegressor</span>

<span class="n">knn_model</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">knn_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># predict on test data</span>
<span class="n">y_pred_knn</span> <span class="o">=</span> <span class="n">knn_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="c1"># evaluate model performance</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">root_mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span>
<span class="n">rmse_knn</span> <span class="o">=</span> <span class="n">root_mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_knn</span><span class="p">)</span>
<span class="n">r2_value_knn</span> <span class="o">=</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_knn</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;K-Nearest Neighbors RMSE: </span><span class="si">{</span><span class="n">rmse_knn</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">, R²: </span><span class="si">{</span><span class="n">r2_value_knn</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>In order to visualize the KNN algorithm on the regression task, we plot the <strong>predictive curve</strong> that maps input values to predicted outputs. This curve shows how K-Nearest Neighbors responds to changes in a single feature. Since KNN is a non-parametric, instance-based method, it doesn’t learn a fixed equation during training. Instead, predictions are based on averaging the target values of the k nearest training examples for any given input.</p>
<p>The resulting predictive curve is typically piecewise-smooth, adapting to local patterns in the data, that is, the curve may bend or flatten in response to regions where data is dense or sparse.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/5-regression-predictive-curve-knn-5.png"><img alt="_images/5-regression-predictive-curve-knn-5.png" src="_images/5-regression-predictive-curve-knn-5.png" style="width: 512px;" />
</a>
</figure>
<p>This makes the predictive curve an especially useful tool for understanding whether KNN is underfitting (<em>e.g.</em>, when k is large) or overfitting (<em>e.g.</em>, when k is small). By adjusting k and observing the changes in the curve’s shape, we can intuitively tune the model’s bias-variance tradeoff.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/5-regression-predictive-curve-knn-1357.png"><img alt="_images/5-regression-predictive-curve-knn-1357.png" src="_images/5-regression-predictive-curve-knn-1357.png" style="width: 512px;" />
</a>
</figure>
</section>
</section>
</section>
</div>
<div class="toctree-wrapper compound">
<span id="document-quick-reference"></span><section id="quick-reference">
<h2>Quick Reference<a class="headerlink" href="#quick-reference" title="Link to this heading"></a></h2>
</section>
<span id="document-guide"></span><section id="instructor-s-guide">
<h2>Instructor’s guide<a class="headerlink" href="#instructor-s-guide" title="Link to this heading"></a></h2>
<section id="why-we-teach-this-lesson">
<h3>Why we teach this lesson<a class="headerlink" href="#why-we-teach-this-lesson" title="Link to this heading"></a></h3>
</section>
<section id="intended-learning-outcomes">
<h3>Intended learning outcomes<a class="headerlink" href="#intended-learning-outcomes" title="Link to this heading"></a></h3>
</section>
<section id="timing">
<h3>Timing<a class="headerlink" href="#timing" title="Link to this heading"></a></h3>
</section>
<section id="preparing-exercises">
<h3>Preparing exercises<a class="headerlink" href="#preparing-exercises" title="Link to this heading"></a></h3>
<p>e.g. what to do the day before to set up common repositories.</p>
</section>
<section id="other-practical-aspects">
<h3>Other practical aspects<a class="headerlink" href="#other-practical-aspects" title="Link to this heading"></a></h3>
</section>
<section id="interesting-questions-you-might-get">
<h3>Interesting questions you might get<a class="headerlink" href="#interesting-questions-you-might-get" title="Link to this heading"></a></h3>
</section>
<section id="typical-pitfalls">
<h3>Typical pitfalls<a class="headerlink" href="#typical-pitfalls" title="Link to this heading"></a></h3>
</section>
</section>
</div>
<div class="toctree-wrapper compound">
</div>
<section id="who-is-the-course-for">
<span id="learner-personas"></span><h2>Who is the course for?<a class="headerlink" href="#who-is-the-course-for" title="Link to this heading"></a></h2>
</section>
<section id="about-the-course">
<h2>About the course<a class="headerlink" href="#about-the-course" title="Link to this heading"></a></h2>
</section>
<section id="see-also">
<h2>See also<a class="headerlink" href="#see-also" title="Link to this heading"></a></h2>
</section>
<section id="credits">
<h2>Credits<a class="headerlink" href="#credits" title="Link to this heading"></a></h2>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, The contributors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>