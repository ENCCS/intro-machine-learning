

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Supervised Learning (I): Classification &mdash; Introduction to Machine Learning  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
      <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx_lesson.css?v=e9df6548" />
      <link rel="stylesheet" type="text/css" href="../_static/term_role_formatting.css?v=4194e21c" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx_rtd_theme_ext_color_contrast.css?v=8e8ea19f" />
      <link rel="stylesheet" type="text/css" href="../_static/overrides.css?v=c88db32d" />

  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=187304be"></script>
      <script src="../_static/doctools.js?v=9a2dae69"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../_static/copybutton.js?v=35a8b989"></script>
      <script src="../_static/minipres.js?v=a0d29692"></script>
      <script>let toggleHintShow = 'Click to show';</script>
      <script>let toggleHintHide = 'Click to hide';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex/" />
    <link rel="search" title="Search" href="../search/" />
    <link rel="next" title="Supervised Learning (II): Regression" href="../05-supervised-ML-regression/" />
    <link rel="prev" title="Scientific Data for Machine Learning" href="../03-scientific-data-for-ML/" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../" class="icon icon-home">
            Introduction to Machine Learning
              <img src="../_static/ENCCS.jpg" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search/" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">The lesson</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../01-intro-to-ML/">Introduction to Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02-fundamentals-of-ML/">Fundamentals of Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03-scientific-data-for-ML/">Scientific Data for Machine Learning</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Supervised Learning (I): Classification</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#importing-dataset">Importing Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="#data-processing">Data Processing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#handling-missing-values-and-outliers">Handling missing values and outliers</a></li>
<li class="toctree-l3"><a class="reference internal" href="#encoding-categorical-variables">Encoding categorical variables</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#data-splitting">Data Splitting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#splitting-features-and-labels">Splitting features and labels</a></li>
<li class="toctree-l3"><a class="reference internal" href="#splitting-training-and-testing-sets">Splitting training and testing sets</a></li>
<li class="toctree-l3"><a class="reference internal" href="#feature-scaling">Feature scaling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#training-model-evaluating-model-performance">Training Model &amp; Evaluating Model Performance</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#k-nearest-neighbors-knn">k-Nearest Neighbors (KNN)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#logistic-regression">Logistic Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#naive-bayes">Naive Bayes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#support-vector-machine-svm">Support Vector Machine (SVM)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#decision-tree">Decision Tree</a></li>
<li class="toctree-l3"><a class="reference internal" href="#random-forest">Random Forest</a></li>
<li class="toctree-l3"><a class="reference internal" href="#gradient-boosting">Gradient Boosting</a></li>
<li class="toctree-l3"><a class="reference internal" href="#multi-layer-perceptron-scikit-learn">Multi-Layer Perceptron (Scikit-Learn)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#deep-neural-networks-keras">Deep Neural Networks (Keras)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#comparison-of-trained-models">Comparison of Trained Models</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../05-supervised-ML-regression/">Supervised Learning (II): Regression</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../quick-reference/">Quick Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../guide/">Instructor’s guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">About</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://enccs.se/lessons/">All lessons</a></li>
<li class="toctree-l1"><a class="reference external" href="https://enccs.se/">ENCCS</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../">Introduction to Machine Learning</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Supervised Learning (I): Classification</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/ENCCS/intro-machine-learning/blob/main/content/04-supervised-ML-classification.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="supervised-learning-i-classification">
<h1>Supervised Learning (I): Classification<a class="headerlink" href="#supervised-learning-i-classification" title="Link to this heading"></a></h1>
<div class="admonition-questions questions admonition" id="questions-0">
<p class="admonition-title">Questions</p>
<ul class="simple">
<li><p>why</p></li>
</ul>
</div>
<div class="admonition-objectives objectives admonition" id="objectives-0">
<p class="admonition-title">Objectives</p>
<ul class="simple">
<li><p>Explain</p></li>
</ul>
</div>
<div class="admonition-instructor-note instructor-note admonition" id="instructor-note-0">
<p class="admonition-title">Instructor note</p>
<ul class="simple">
<li><p>40 min teaching</p></li>
<li><p>40 min exercises</p></li>
</ul>
</div>
<p>Classification is a supervised ML task where the model predicts discrete class labels based on input features. It involves training a model on labeled data so that it can assign new data to predefined categories or classes based on patterns learned from labeled training data.</p>
<p>In binary classification, models predict one of two classes, such as spam or not spam for emails. Multiclass classification extends this to multiple categories, like classifying images as cats, dogs, or birds.</p>
<p>Common algorithms for classification task include k-Nearest Neighbors (KNN), Logistic Regression, Naive Bayes, Support Vector Machine (SVM), Decision Tree, Random Forest, Gradient Boosting, and Neural Networks.</p>
<p>In this episode we will perform supervised classification tasks to categorize penguins into three species – Adelie, Chinstrap, and Gentoo – based on their physical measurements (flipper length, body mass, <em>etc.</em>). We will build and train multiple classifier models as mentioned above. Each model will be evaluated using appropriate performance metrics like accuracy, precision, recall, and F1 score. By comparing the results across models, we aim to identify which classifier model provides the most accurate and reliable classification for this task.</p>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="../_images/4-penguins-categories.png"><img alt="../_images/4-penguins-categories.png" src="../_images/4-penguins-categories.png" style="width: 640px;" />
</a>
<figcaption>
<p><span class="caption-text">The Palmer Penguins data were collected from 2007-2009 by Dr. Kristen Gorman with the <a class="reference external" href="https://lternet.edu/site/palmer-antarctica-lter/">Palmer Station Long Term Ecological Research Program</a>, part of the <a class="reference external" href="https://lternet.edu/">US Long Term Ecological Research Network</a>. The data were imported directly from the <a class="reference external" href="https://edirepository.org/">Environmental Data Initiative (EDI)</a> Data Portal, and are available for use by CC0 license (“No Rights Reserved”) in accordance with the <a class="reference external" href="https://lternet.edu/data-access-policy/">Palmer Station Data Policy</a>.</span><a class="headerlink" href="#id3" title="Link to this image"></a></p>
</figcaption>
</figure>
<section id="importing-dataset">
<h2>Importing Dataset<a class="headerlink" href="#importing-dataset" title="Link to this heading"></a></h2>
<p>Seaborn provides the Penguins dataset through its built-in data-loading functions. We can access it using <code class="docutils literal notranslate"><span class="pre">sns.load_dataset('penguin')</span></code> and then have a quick look at the data:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

<span class="n">penguins</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;penguins&#39;</span><span class="p">)</span>
<span class="n">penguins</span>
</pre></div>
</div>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td></td>
<td><p>species</p></td>
<td><p>island</p></td>
<td><p>bill_length_mm</p></td>
<td><p>bill_depth_mm</p></td>
<td><p>flipper_length_mm</p></td>
<td><p>body_mass_g</p></td>
<td><p>sex</p></td>
</tr>
<tr class="row-even"><td><p>0</p></td>
<td><p>Adelie</p></td>
<td><p>Torgersen</p></td>
<td><p>39.1</p></td>
<td><p>18.7</p></td>
<td><p>181.0</p></td>
<td><p>3750.0</p></td>
<td><p>Male</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>Adelie</p></td>
<td><p>Torgersen</p></td>
<td><p>39.5</p></td>
<td><p>17.4</p></td>
<td><p>186.0</p></td>
<td><p>3800.0</p></td>
<td><p>Female</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>Adelie</p></td>
<td><p>Torgersen</p></td>
<td><p>40.3</p></td>
<td><p>18.0</p></td>
<td><p>195.0</p></td>
<td><p>3250.0</p></td>
<td><p>Female</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>Adelie</p></td>
<td><p>Torgersen</p></td>
<td><p>NaN</p></td>
<td><p>NaN</p></td>
<td><p>NaN</p></td>
<td><p>NaN</p></td>
<td><p>NaN</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>Adelie</p></td>
<td><p>Torgersen</p></td>
<td><p>36.7</p></td>
<td><p>19.3</p></td>
<td><p>193.0</p></td>
<td><p>3450.0</p></td>
<td><p>Female</p></td>
</tr>
<tr class="row-odd"><td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
<td><p>…</p></td>
</tr>
<tr class="row-even"><td><p>339</p></td>
<td><p>Gentoo</p></td>
<td><p>Biscoe</p></td>
<td><p>NaN</p></td>
<td><p>NaN</p></td>
<td><p>NaN</p></td>
<td><p>NaN</p></td>
<td><p>NaN</p></td>
</tr>
<tr class="row-odd"><td><p>340</p></td>
<td><p>Gentoo</p></td>
<td><p>Biscoe</p></td>
<td><p>46.8</p></td>
<td><p>14.3</p></td>
<td><p>215.0</p></td>
<td><p>4850.0</p></td>
<td><p>Female</p></td>
</tr>
<tr class="row-even"><td><p>341</p></td>
<td><p>Gentoo</p></td>
<td><p>Biscoe</p></td>
<td><p>50.4</p></td>
<td><p>15.7</p></td>
<td><p>222.0</p></td>
<td><p>5750.0</p></td>
<td><p>Male</p></td>
</tr>
<tr class="row-odd"><td><p>342</p></td>
<td><p>Gentoo</p></td>
<td><p>Biscoe</p></td>
<td><p>45.2</p></td>
<td><p>14.8</p></td>
<td><p>212.0</p></td>
<td><p>5200.0</p></td>
<td><p>Female</p></td>
</tr>
<tr class="row-even"><td><p>343</p></td>
<td><p>Gentoo</p></td>
<td><p>Biscoe</p></td>
<td><p>49.9</p></td>
<td><p>16.1</p></td>
<td><p>213.0</p></td>
<td><p>5400.0</p></td>
<td><p>Male</p></td>
</tr>
</tbody>
</table>
<p>There are seven columns include:</p>
<ul class="simple">
<li><p><em>species</em>: penguin species (Adelie, Chinstrap, Gentoo)</p></li>
<li><p><em>island</em>: island where the penguin was found (Biscoe, Dream, Torgersen)</p></li>
<li><p><em>bill_length_mm</em>: length of the bill</p></li>
<li><p><em>bill_depth_mm</em>: depth of the bill</p></li>
<li><p><em>flipper_length_mm</em>: length of the flipper</p></li>
<li><p><em>body_mass_g</em>: body mass in grams</p></li>
<li><p><em>sex</em>: male or female</p></li>
</ul>
<p>Looking at numbers from <code class="docutils literal notranslate"><span class="pre">penguins</span></code> and <code class="docutils literal notranslate"><span class="pre">penguins.describe()</span></code> usually does not give a very good intuition about the data we are working with, we have the preference to visualize the data.</p>
<p>One nice visualization for datasets with relatively few attributes is the Pair Plot, which can be created using <code class="docutils literal notranslate"><span class="pre">sns.pairplot(...)</span></code>. It shows a scatterplot of each attribute plotted against each of the other attributes. By using the <code class="docutils literal notranslate"><span class="pre">hue='species'</span></code> setting for the pairplot the graphs on the diagonal are layered kernel density estimate plots for the different values of the <code class="docutils literal notranslate"><span class="pre">species</span></code> column.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">penguins_classification</span><span class="p">[[</span><span class="s2">&quot;species&quot;</span><span class="p">,</span> <span class="s2">&quot;bill_length_mm&quot;</span><span class="p">,</span> <span class="s2">&quot;bill_depth_mm&quot;</span><span class="p">,</span> <span class="s2">&quot;flipper_length_mm&quot;</span><span class="p">,</span> <span class="s2">&quot;body_mass_g&quot;</span><span class="p">]],</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;species&quot;</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/4-penguins-pairplot.png"><img alt="../_images/4-penguins-pairplot.png" src="../_images/4-penguins-pairplot.png" style="width: 640px;" />
</a>
</figure>
<div class="admonition-discussion exercise important admonition" id="exercise-0">
<p class="admonition-title">Discussion</p>
<p>Take a look at the pairplot we created. Consider the following questions:</p>
<ul class="simple">
<li><p>Is there any class that is easily distinguishable from the others?</p></li>
<li><p>Which combination of attributes shows the best separation for all 3 class labels at once?</p></li>
<li><p>(optional) Create a similar pairplot, but with <code class="docutils literal notranslate"><span class="pre">hue=&quot;sex&quot;</span></code>. Explain the patterns you see. Which combination of features distinguishes the two sexes best?</p></li>
</ul>
<div class="admonition-solution solution important dropdown admonition" id="solution-0">
<p class="admonition-title">Solution</p>
<ol class="arabic simple">
<li><p>The plots show that the green class (Gentoo) is somewhat more easily distinguishable from the other two.</p></li>
<li><p>Adelie and Chinstrap seem to be separable by a combination of bill length and bill depth (other combinations are also possible such as bill length and flipper length).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sns.pairplot(penguins_classification,</span> <span class="pre">hue=&quot;sex&quot;,</span> <span class="pre">height=2.0)</span></code>. From the plots you can see that for each species females have smaller bills and flippers, as well as a smaller body mass. You would need a combination of the species and the numerical features to successfully distinguish males from females. The combination of bill_depth_mm and body_mass_g gives the best separation.</p></li>
</ol>
</div>
</div>
</section>
<section id="data-processing">
<h2>Data Processing<a class="headerlink" href="#data-processing" title="Link to this heading"></a></h2>
<section id="handling-missing-values-and-outliers">
<h3>Handling missing values and outliers<a class="headerlink" href="#handling-missing-values-and-outliers" title="Link to this heading"></a></h3>
<p>For a ML task, the input data (features) and target data (label) are not yet in a right format to use. We need to pre-process the data (as what we did yesterday) to clean missing values using <code class="docutils literal notranslate"><span class="pre">penguins_classification</span> <span class="pre">=</span> <span class="pre">penguins.dropna()</span></code> and check duplicate values using <code class="docutils literal notranslate"><span class="pre">penguins_classification.duplicated().value_counts()</span></code>.</p>
<p>It is noted that we don’t have outliers in this dataset (as we have discussed this issue in the <a href="#id10"><span class="problematic" id="id11">`data processing &lt;&gt;`_</span></a> tutorial). For the other datasets you use for the first time, you should check if there are outliers for some features in the dataset, and then take steps to handle the outliers, either to imputate outliers with mean/median values or to remove abnormal outliers for simplicity.</p>
</section>
<section id="encoding-categorical-variables">
<h3>Encoding categorical variables<a class="headerlink" href="#encoding-categorical-variables" title="Link to this heading"></a></h3>
<p>In the classification task, we will use the categorical variable <em>species</em> as the label (target variable), and other columns as features to predict the species of penguins.</p>
<div class="admonition-discussion exercise important admonition" id="exercise-1">
<p class="admonition-title">Discussion</p>
<ul class="simple">
<li><p>why to use <em>species</em>?</p></li>
<li><p>why not to use other other categorical variables (here it would be <em>island</em> and <em>sex</em>)?</p></li>
</ul>
<div class="admonition-solution solution important dropdown admonition" id="solution-1">
<p class="admonition-title">Solution</p>
<ol class="arabic simple">
<li><p><em>species</em> will be the main biological classification target in this dataset as it 3 distinct classes (Adelie, Chinstrap, and Gentoo). This is commonly used in ML tutorials as a multi-class classification example (similar to the <a class="reference external" href="https://archive.ics.uci.edu/dataset/53/iris">Iris dataset</a>).</p></li>
<li><p><em>island</em> is not a ideal label as it is just geographical info, not a biological classification target; <em>sex</em> is possible but quite limited. This variable only has two classes (only for binary classification), and the data is unbalanced and has missing values.</p></li>
</ol>
</div>
</div>
<p>It is noted that ML models cannot directly process categorical (non-numeric) data, so we have to encode categorical variables like <em>species</em>, <em>island</em>, and <em>sex</em> into numerical values. Here we use <code class="docutils literal notranslate"><span class="pre">LabelEncoder</span></code> from <code class="docutils literal notranslate"><span class="pre">sklearn.preprocessing</span></code> to convert the species column, which serves as our classification target. The <code class="docutils literal notranslate"><span class="pre">LabelEncoder</span></code> assigns a unique integer to each species: “Adelie” becomes 0, “Chinstrap” becomes 1, and “Gentoo” becomes 2. This transformation allows classification algorithms to treat the species labels as distinct, unordered classes.</p>
<p>Then we apply the same rule to encode the island and sex columns. Although these are typically better handled with one-hot encoding due to their nominal nature, we use <code class="docutils literal notranslate"><span class="pre">LabelEncoder</span></code> here for simplicity and compact representation. Each unique category in island (<em>e.g.</em>, “Biscoe”, “Dream”, “Torgersen”) and sex (<em>e.g.</em>, “Male”, “Female”) is mapped to a unique integer. This enables us to include them as input features in the model without manual transformation. However, it’s important to note that <code class="docutils literal notranslate"><span class="pre">LabelEncoder</span></code> introduces an implicit ordinal relationship, which might not always be appropriate – in such cases, <code class="docutils literal notranslate"><span class="pre">OneHotEncoder</span></code> is preferred.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">LabelEncoder</span>

<span class="n">encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>

<span class="c1"># encode &quot;species&quot; column with 0=Adelie, 1=Chinstrap, and 2=Gentoo</span>
<span class="n">penguins_classification</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s1">&#39;species&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">penguins_classification</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">])</span>

<span class="c1"># encode &quot;island&quot; column with 0=Biscoe, 1=Dream and 2=Torgersen</span>
<span class="n">penguins_classification</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s1">&#39;island&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">penguins_classification</span><span class="p">[</span><span class="s1">&#39;island&#39;</span><span class="p">])</span>

<span class="c1"># encode &quot;sex&quot; with column 0=Female and 1=Male</span>
<span class="n">penguins_classification</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="s1">&#39;sex&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">penguins_classification</span><span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">])</span>
</pre></div>
</div>
</section>
</section>
<section id="data-splitting">
<h2>Data Splitting<a class="headerlink" href="#data-splitting" title="Link to this heading"></a></h2>
<section id="splitting-features-and-labels">
<h3>Splitting features and labels<a class="headerlink" href="#splitting-features-and-labels" title="Link to this heading"></a></h3>
<p>In preparing the penguins dataset for classification, we first need to split the data into features and labels. The target variable we aim to predict is the penguin species, which we encode into numeric labels using <code class="docutils literal notranslate"><span class="pre">LabelEncoder</span></code>. This encoded species column will be the <strong>label vector</strong> (<em>e.g.</em>, <strong>y</strong>). The remaining columns – such as bill length, bill depth, flipper length, body mass, and encoded categorical variables like island and sex – constitute the <strong>feature matrix</strong> (<em>e.g.</em>, <strong>X</strong>). These features contain the input information the model will learn from.</p>
<p>Separating features (X) from labels (y) ensures a clear distinction between what the model uses for prediction and what it is trying to predict.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">penguins_classification</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;species&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">penguins_classification</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;int&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="splitting-training-and-testing-sets">
<h3>Splitting training and testing sets<a class="headerlink" href="#splitting-training-and-testing-sets" title="Link to this heading"></a></h3>
<p>After separating features and labels in the penguins dataset, we further divide the data into a training set and a testing set. The training set is used to train the model, allowing it to learn patterns and relationships from the data, and the test set, on the other hand, is reserved for evaluating the model’s performance on unseen data. A common split is 80% for training and 20% for testing, which provides enough data for training while still retaining a meaningful test set.</p>
<p>This splitting is typically done using the <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code> function from <code class="docutils literal notranslate"><span class="pre">sklearn.model_selection</span></code>, with a fixed <code class="docutils literal notranslate"><span class="pre">random_state</span></code> to ensure reproducibility.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of examples for training is </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span><span class="si">}</span><span class="s2"> and test is </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="feature-scaling">
<h3>Feature scaling<a class="headerlink" href="#feature-scaling" title="Link to this heading"></a></h3>
<p>Before training, it is also essential to ensure that numerical features are properly scaled via applying standardization or normalization – especially for distance-based or gradient-based models – to achieve optimal results.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># Standardize features</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>

<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="training-model-evaluating-model-performance">
<h2>Training Model &amp; Evaluating Model Performance<a class="headerlink" href="#training-model-evaluating-model-performance" title="Link to this heading"></a></h2>
<p>After preparing the Penguins dataset by handling missing values, encoding categorical variables, and splitting it into features-labels and training-test datasets, the next step is to apply classification algorithms including k-Nearest Neighbors (KNN), Naive Bayes, Decision Trees, Random Forests, and Neural Networks to predict penguin species based on their physical measurements. Each algorithm offers a unique approach to pattern recognition and generalization, and applying them to the same prepared dataset allows for a fair comparison of their predictive performance.</p>
<p>Below is the generic steps for representative algorithms we will use to training a model for penguins classification:</p>
<ul class="simple">
<li><p>choosing a model class and importing that model <code class="docutils literal notranslate"><span class="pre">from</span> <span class="pre">sklearn.neighbors</span> <span class="pre">import</span> <span class="pre">XXX</span></code></p></li>
<li><p>choosing the model hyperparameters by instantiating this class with desired values <code class="docutils literal notranslate"><span class="pre">xxx_model</span> <span class="pre">=</span> <span class="pre">XXX(&lt;...</span> <span class="pre">hyperparameters</span> <span class="pre">...&gt;)</span></code></p></li>
<li><p>training the model to the preprocessed train data by calling the <code class="docutils literal notranslate"><span class="pre">fit()</span></code> method of the model instance <code class="docutils literal notranslate"><span class="pre">xxx_model.fit(X_train_scaled,</span> <span class="pre">y_train)</span></code></p></li>
<li><p>making predictions using the trained model on test data <code class="docutils literal notranslate"><span class="pre">y_pred_xxx</span> <span class="pre">=</span> <span class="pre">xxx_model.predict(X_test_scaled)</span></code></p></li>
<li><p>evaluating model’s performance using available metrics <code class="docutils literal notranslate"><span class="pre">score_xxx</span> <span class="pre">=</span> <span class="pre">accuracy_score(y_test,</span> <span class="pre">y_pred_xxx)</span></code></p></li>
<li><p>(optional) data visualization of confusion matrix and relevant data</p></li>
</ul>
<section id="k-nearest-neighbors-knn">
<h3>k-Nearest Neighbors (KNN)<a class="headerlink" href="#k-nearest-neighbors-knn" title="Link to this heading"></a></h3>
<p>One intuitive and widely-used method is the KNN algorithm. KNN is a non-parametric, instance-based algorithm that predicts a sample’s label based on the majority class of its <em>k</em> closest neighbors in training set. <strong>KNN does not require training in the traditional sense; instead, it stores the entire dataset and performs computation during prediction time. This makes it a lazy learner but potentially expensive during inference.</strong></p>
<p>Here is an example of using the KNN algorithm to determine which class the new point belongs to. When the given query point, the KNN algorithm calculates the distance between this point and all points in the training dataset. It then selects the <em>k</em> points that are closest. The class with the most representatives among the <em>k</em> neighbors is chosen to be the prediction result for the query point. It is noted that the choice of <em>k</em> (the number of neighbors) significantly affects performance: a small <em>k</em> may be sensitive to noise, while a large <em>k</em> may smooth over important patterns.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/4-knn-example.png"><img alt="../_images/4-knn-example.png" src="../_images/4-knn-example.png" style="width: 640px;" />
</a>
</figure>
<p>Let’s create the KNN model. Here we choose 3 as the <em>k</em> value of the algorithm, which means that data needs 3 neighbors to be classified as one entity. Then we fit the train data using the <code class="docutils literal notranslate"><span class="pre">fit()</span></code> method.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.neighbors</span><span class="w"> </span><span class="kn">import</span> <span class="n">KNeighborsClassifier</span>

<span class="n">knn_model</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">knn_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
<p>After we fitting the training data, we use the trained model to predict species on the test set and evaluate its performance.</p>
<p>For classification tasks, metrics like accuracy, precision, recall, and the F1-score provide a comprehensive view of model performance.</p>
<ul class="simple">
<li><p><strong>accuracy</strong> measures the proportion of correctly classified instances across all species (Adelie, Chinstrap, Gentoo), and it gives an overall measure of how often the model is correct, but it can be misleading for imbalanced datasets.</p></li>
<li><p><strong>precision</strong> quantifies the proportion of correct positive predictions for each species, while <strong>recall</strong> assesses the proportion of actual positives correctly identified.</p></li>
<li><p>the <strong>F1-score</strong>, the harmonic mean of precision and recall, balances these metrics for each class, especially useful given the dataset’s imbalanced species distribution.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># predict on test data</span>
<span class="n">y_pred_knn</span> <span class="o">=</span> <span class="n">knn_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="c1"># evaluate model performance</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">accuracy_score</span>

<span class="n">score_knn</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_knn</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy for k-Nearest Neighbors:&quot;</span><span class="p">,</span> <span class="n">score_knn</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_knn</span><span class="p">))</span>
</pre></div>
</div>
<p>In classification tasks, a <strong>confusion matrix</strong> is a valuable tool for evaluating model performance by comparing predicted labels against true labels. For a multiclass classification task like the penguins dataset, the confusion matrix is an <strong>N x N</strong> matrix, where <strong>N</strong> is the number of target classes (here <strong>N=3</strong> for three penguins species). Each cell <em>(i, j)</em> in the matrix indicates the number of instances where the true class was <em>i</em> and the model predicted class <em>j</em>. Diagonal elements represent correct predictions, while off-diagonal elements indicate misclassifications. The confusion matrix provides an easy-to-understand overview of how often the predictions match the actual labels and where the model tends to make mistakes.</p>
<p>Since we will plot the confusion matrix multiple times, we write a function and call this function later whenever needed, which promotes clarity and avoids redundancy. This is especially helpful as we evaluate multiple classifiers such as KNN, Decision Trees, or SVM on the penguins dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">confusion_matrix</span>

<span class="k">def</span><span class="w"> </span><span class="nf">plot_confusion_matrix</span><span class="p">(</span><span class="n">conf_matrix</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">fig_name</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">conf_matrix</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;d&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;OrRd&#39;</span><span class="p">,</span>
                <span class="n">xticklabels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Adelie&quot;</span><span class="p">,</span> <span class="s2">&quot;Chinstrap&quot;</span><span class="p">,</span> <span class="s2">&quot;Gentoo&quot;</span><span class="p">],</span>
                <span class="n">yticklabels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Adelie&#39;</span><span class="p">,</span> <span class="s1">&#39;Chinstrap&#39;</span><span class="p">,</span> <span class="s1">&#39;Gentoo&#39;</span><span class="p">],</span> <span class="n">cbar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Predicted Label&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;True Label&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">fig_name</span><span class="p">)</span>
</pre></div>
</div>
<p>We compute the confusion matrix from the trined model using the KNN algorithm, and visualize the matrix.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cm_knn</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_knn</span><span class="p">)</span>

<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm_knn</span><span class="p">,</span> <span class="s2">&quot;Confusion Matrix using KNN algorithm&quot;</span><span class="p">,</span> <span class="s2">&quot;confusion-matrix-knn.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="../_images/4-confusion-matrix-knn.png"><img alt="../_images/4-confusion-matrix-knn.png" src="../_images/4-confusion-matrix-knn.png" style="width: 420px;" />
</a>
<figcaption>
<p><span class="caption-text">The first row: there are 28 Adelie penguins in the test data, and all these penguins are identified as Adelie (valid). The second row: there are 20 Chinstrap pengunis in the test data, with 2 identified as Adelie (invalid), and 18 identified as Chinstrap (valid). The third row: there are 19 Gentoo penguins in the test data, and all these penguins are identified as Gentoo (valid).</span><a class="headerlink" href="#id4" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="logistic-regression">
<h3>Logistic Regression<a class="headerlink" href="#logistic-regression" title="Link to this heading"></a></h3>
<p><strong>Logistic Regression</strong> is a fundamental classification algorithm to predict categorical outcomes. Despite its name, logistic regression is not a regression algorithm but a classification method that predicts the probability of an instance belonging to a particular class.</p>
<p>For binary classification, it uses the logistic (<strong>sigmoid</strong>) function to map a linear combination of input features to a probability between 0 and 1, which is then thresholded (typically at 0.5) to assign a class.</p>
<p>For a multiclass classification, logistic regression can be extended using strategies like <strong>one-vs-rest</strong> (OvR) or softmax regression.</p>
<ul class="simple">
<li><p>in OvR, a separate binary classifier is trained for each species against all others.</p></li>
<li><p><strong>softmax regression</strong> generalizes the logistic function to compute probabilities across all classes simultaneously, selecting the class with the highest probability.</p></li>
</ul>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="../_images/4-logistic-regression-example.png"><img alt="../_images/4-logistic-regression-example.png" src="../_images/4-logistic-regression-example.png" style="width: 640px;" />
</a>
<figcaption>
<p><span class="caption-text">(Upper left) the sigmoid function; (upper middle) the softmax regression process: three input features to the softmax regression model resulting in three output vectors where each contains the predicted probabilities for three possible classes; (upper right) a bar chart of softmax outputs in which each group of bars represents the predicted probability distribution over three classes; lower subplots) three binary classifiers distinguish one class from the other two classes using the one-vs-rest approach.</span><a class="headerlink" href="#id5" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>The creation of a Logistic Regression model and the process of fitting it to the training data are nearly identical to those used for the KNN model described above, except that a different classifier is selected. The code example and the resulting confusion matrix plot are provided below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="n">lr_model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span> <span class="o">=</span> <span class="mi">123</span><span class="p">)</span>
<span class="n">lr_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred_lr</span> <span class="o">=</span> <span class="n">lr_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="n">score_lr</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_lr</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy for Logistic Regression:&quot;</span><span class="p">,</span> <span class="n">score_lr</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_lr</span><span class="p">))</span>

<span class="n">cm_lr</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_lr</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm_lr</span><span class="p">,</span> <span class="s2">&quot;Confusion Matrix using Logistic Regression algorithm&quot;</span><span class="p">,</span> <span class="s2">&quot;confusion-matrix-lr.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/4-confusion-matrix-lr.png"><img alt="../_images/4-confusion-matrix-lr.png" src="../_images/4-confusion-matrix-lr.png" style="width: 420px;" />
</a>
</figure>
</section>
<section id="naive-bayes">
<h3>Naive Bayes<a class="headerlink" href="#naive-bayes" title="Link to this heading"></a></h3>
<p>The <strong>Naive Bayes</strong> algorithm is a simple yet powerful probabilistic classifier based on Bayes’ Theorem. This classifier assumes that all features are equally important and independent which is often not the case and may result in some bias. However, the assumption of independence simplifies the computations by turning conditional probabilities into products of probabilities. This algorithm computes the probability of each class given the input features and selects the class with the highest posterior probability.</p>
<p>Logistic regression and Naive Bayes are both popular algorithms for classification tasks, but they differ significantly in their approach, assumptions, and mechanics.</p>
<ul class="simple">
<li><p>Logistic regression is a <strong>discriminative</strong> model that directly models the probability of a data point belonging to a particular class by fitting a linear combination of features through a logistic (sigmoid) function for binary classification or softmax for multiclass tasks. For the penguins dataset, it would use features like bill length and flipper length to compute a weighted sum, transforming it into probabilities for species like Adelie, Chinstrap, or Gentoo. It assumes a linear relationship between features and the log-odds of the classes and optimizes parameters using maximum likelihood estimation, making it sensitive to feature scaling and correlations. Logistic regression is robust to noise and can handle correlated features to some extent, but it may struggle with highly non-linear relationships unless feature engineering is applied.</p></li>
<li><p>Naive Bayes, in contrast, is a <strong>generative</strong> model that relies on Bayes’ theorem to compute the probability of a class given the features, assuming conditional independence between features given the class. For the penguins dataset, it would estimate the likelihood of features (<em>e.g.</em>, bill depth) for each species and combine these with prior probabilities to predict the most likely species. The “naive” assumption of feature independence often doesn’t hold (<em>e.g.</em>, bill length and depth may be correlated), but Naive Bayes is computationally efficient, works well with high-dimensional data, and is less sensitive to irrelevant features. However, it can underperform when feature dependencies are significant or when the data distribution deviates from its assumptions (<em>e.g.</em>, Gaussian for continuous features in Gaussian Naive Bayes). Unlike logistic regression, it doesn’t require feature scaling but may need careful handling of zero probabilities (<em>e.g.</em>, via smoothing).</p></li>
</ul>
<p>Below is an example comparing Logistic Regression and Naive Bayes decision boundaries on a synthetic dataset having two features. The visualization highlights their fundamental differences in modeling assumptions and classification behavior: <strong>Logistic Regression learns a linear decision boundary directly, while Naive Bayes models feature distributions per class (assuming independence)</strong>.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/4-naive-bayes-example.png"><img alt="../_images/4-naive-bayes-example.png" src="../_images/4-naive-bayes-example.png" style="width: 640px;" />
</a>
</figure>
<p>To apply Naive Bayes, we use <code class="docutils literal notranslate"><span class="pre">GaussianNB</span></code> from <code class="docutils literal notranslate"><span class="pre">sklearn.naive_bayes</span></code>, which assumes that the features follow a Gaussian (normal) distribution, which is an appropriate choice for continuous numerical data such as bill length and body mass. Since Naive Bayes relies on probabilities, <strong>feature scaling is not required</strong>, but <strong>handling missing values and encoding categorical variables numerically is still necessary</strong>.</p>
<p>While Naive Bayes may not outperform more complex models like Random Forests, it offers <strong>fast training, low memory usage</strong>, and good performance for simple tasks.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.naive_bayes</span><span class="w"> </span><span class="kn">import</span> <span class="n">GaussianNB</span>

<span class="n">nb_model</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="n">nb_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred_nb</span> <span class="o">=</span> <span class="n">nb_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="n">score_nb</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_nb</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy for Naive Bayes:&quot;</span><span class="p">,</span> <span class="n">score_nb</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_nb</span><span class="p">))</span>

<span class="n">cm_nb</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_nb</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm_nb</span><span class="p">,</span> <span class="s2">&quot;Confusion Matrix using Naive Bayes algorithm&quot;</span><span class="p">,</span> <span class="s2">&quot;confusion-matrix-nb.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/4-confusion-matrix-nb.png"><img alt="../_images/4-confusion-matrix-nb.png" src="../_images/4-confusion-matrix-nb.png" style="width: 420px;" />
</a>
</figure>
</section>
<section id="support-vector-machine-svm">
<h3>Support Vector Machine (SVM)<a class="headerlink" href="#support-vector-machine-svm" title="Link to this heading"></a></h3>
<p>Previously we shown an example using Logistic Regression classifier producing a linear decision boundary that separates cats from dogs. It works by fitting a linear decision boundary that separates two classes based on the logistic function, making it particularly effective when the data is linearly separable. One characteristic of logistic regression is that the decision boundary tends to fall in the region where the probabilities of two classes are closest – typically where the model is most uncertain.</p>
<p>However, when there exists a large gap between two well-separated classes – as often occurs when distinguishing cats and dogs based on weight and ear length – logistic regression faces an inherent limitation: infinite possible solutions. The algorithm has no mechanism to select an “optimal” boundary when multiple valid linear separators exist in the wide margin between classes, and it will place the decision boundary somewhere in that gap, leading to a broad, undefined decision region with no supporting data. While this may not affect accuracy on clearly separated data, it can make the model less robust when new or noisy data appears near that boundary.</p>
<p>Below is an example, again, to separate cats from dogs based on ear length and weight. Besides the linear decision boundary from Logistic Regression classifier, we can find three additional linear boundaries that can also have a good separation of cats from dogs. Which one is better than the others and how to evaluate their performance on unseen data?</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/4-svm-example-large-gap.png"><img alt="../_images/4-svm-example-large-gap.png" src="../_images/4-svm-example-large-gap.png" style="width: 640px;" />
</a>
</figure>
<p>To better handle such situation, we can transition to the <strong>Support Vector Machine</strong> (SVM) algorithm. SVM takes a different approach by focusing on the concept of maximizing the margin – the distance between the decision boundary and the closest data points from each class (the support vectors) (as shown in the figure below). When there is a large gap between the two classes, SVM utilizes that space effectively by pushing the boundary toward the center of the gap while maintaining the maximum margin. This leads to a more stable and robust classifier, particularly in cases where the classes are well-separated.</p>
<p>Unlike Logistic Regression, which uses all data points to estimate probabilities, SVM relies primarily on the most critical examples (the ones nearest the boundary), making it less sensitive to outliers and more precise in defining class divisions.</p>
<figure class="align-center" id="id6">
<a class="reference internal image-reference" href="../_images/4-svm-example-with-max-margin-separation.png"><img alt="../_images/4-svm-example-with-max-margin-separation.png" src="../_images/4-svm-example-with-max-margin-separation.png" style="width: 640px;" />
</a>
<figcaption>
<p><span class="caption-text">The SVM classification boundary for distinguishing cats and dogs based on ear length and weight. The solid black line represents the maximum margin hyperplane (decision boundary), while the dashed green lines show the positive and negative hyperplanes that define the margin. Black circles highlight the support vectors - the critical data points that determine the margin width.</span><a class="headerlink" href="#id6" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>To apply SVM, we use <code class="docutils literal notranslate"><span class="pre">SVC</span></code> (Support Vector Classification) from <code class="docutils literal notranslate"><span class="pre">sklearn.svm</span></code>, which by default assumes that the features follow a nonlinear relationship modeled by the <code class="docutils literal notranslate"><span class="pre">rbf</span></code> (Radial Basis Function) kernel. This kernel allows the model to find complex decision boundaries by implicitly mapping the input features into a higher-dimensional space. You can easily change the kernel to <code class="docutils literal notranslate"><span class="pre">linear</span></code>, <code class="docutils literal notranslate"><span class="pre">poly</span></code>, or <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> to experiment with different decision boundaries.</p>
<p>By adjusting the hyperparameters such as <code class="docutils literal notranslate"><span class="pre">C</span></code> (regularization strength) and <code class="docutils literal notranslate"><span class="pre">gamma</span></code> (kernel coefficient), we can control the trade-off between the margin width and classification accuracy. Below is a code example demonstrating how to use SVC with the RBF kernel for the penguins classification task.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.svm</span><span class="w"> </span><span class="kn">import</span> <span class="n">SVC</span>

<span class="n">svm_model</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">svm_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred_svm</span> <span class="o">=</span> <span class="n">svm_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="n">score_svm</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_svm</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy for Support Vector Machine:&quot;</span><span class="p">,</span> <span class="n">score_svm</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_svm</span><span class="p">))</span>

<span class="n">cm_svm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_svm</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm_svm</span><span class="p">,</span> <span class="s2">&quot;Confusion Matrix using Support Vector Machine algorithm&quot;</span><span class="p">,</span> <span class="s2">&quot;confusion-matrix-svm.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/4-confusion-matrix-svm.png"><img alt="../_images/4-confusion-matrix-svm.png" src="../_images/4-confusion-matrix-svm.png" style="width: 420px;" />
</a>
</figure>
</section>
<section id="decision-tree">
<h3>Decision Tree<a class="headerlink" href="#decision-tree" title="Link to this heading"></a></h3>
<p><strong>Decision Tree</strong> algorithm is a versatile and interpretable method for classification tasks. The core idea of this algorithm is to recursively split the dataset into smaller subsets based on feature thresholds creating a tree-like structure of decisions that result in the most significant separation of target classes.</p>
<p>Here is one example showing how to separate cats and dogs on the basis of two or three features.</p>
<figure class="align-center" id="id7">
<a class="reference internal image-reference" href="../_images/4-decision-tree-example.png"><img alt="../_images/4-decision-tree-example.png" src="../_images/4-decision-tree-example.png" style="width: 640px;" />
</a>
<figcaption>
<p><span class="caption-text">(Upper) decision boundary separating cats and dogs based on two features (ear length and weight), and the corresponding decision tree structure; (lower): two decision boundaries separating cats and dogs based on three features (ear length, weight, and tail length), and the corresponding decision tree structure.</span><a class="headerlink" href="#id7" title="Link to this image"></a></p>
</figcaption>
</figure>
<p>The code example for the Decision Tree classifier is provided below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.tree</span><span class="w"> </span><span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="n">dt_model</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">123</span><span class="p">)</span>
<span class="n">dt_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred_dt</span> <span class="o">=</span> <span class="n">dt_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="n">score_dt</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_dt</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy for Decision Tree:&quot;</span><span class="p">,</span> <span class="n">score_dt</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_dt</span><span class="p">))</span>

<span class="n">cm_dt</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_dt</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm_dt</span><span class="p">,</span> <span class="s2">&quot;Confusion Matrix using Decision Tree algorithm&quot;</span><span class="p">,</span> <span class="s2">&quot;confusion-matrix-dt.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/4-confusion-matrix-dt.png"><img alt="../_images/4-confusion-matrix-dt.png" src="../_images/4-confusion-matrix-dt.png" style="width: 420px;" />
</a>
</figure>
<p>We visualize the Decision Tree structure to understand how penguins are classified based on their physical characteristics.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.tree</span><span class="w"> </span><span class="kn">import</span> <span class="n">plot_tree</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plot_tree</span><span class="p">(</span><span class="n">dt_model</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">filled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rounded</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Decision Tree Structure for Penguins Species Classification&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/4-decision-tree-structure.png"><img alt="../_images/4-decision-tree-structure.png" src="../_images/4-decision-tree-structure.png" style="width: 640px;" />
</a>
</figure>
</section>
<section id="random-forest">
<h3>Random Forest<a class="headerlink" href="#random-forest" title="Link to this heading"></a></h3>
<p>While decision trees are easy to interpret and visualize, they come with some notable drawbacks. One of the primary issues is their tendency to overfit the training data, especially when the tree is allowed to grow deep without constraints like maximum depth or minimum samples per split. This leads to a model that captures noise in the training data, leading to poor generalization on unseen data, such as misclassifying a Gentoo penguin as Chinstrap due to overly specific splits. Additionally, decision trees are sensitive to small variations in the data – a slight change (<em>e.g.</em>, a few noisy measurements) in the dataset can result in a significantly different tree structure, reducing model stability and reliability.</p>
<p>To address these limitations, we can use an ensemble learning technique called <strong>Random Forest</strong>. A random forest builds upon the idea of decision trees by creating a large collection of them, each trained on a randomly selected subset of the data and features to produce a more accurate and stable prediction. By averaging the predictions of many trees (through majority voting for classification), random forest reduces overfitting, improves generalization, and mitigates the instability of individual trees.</p>
<p>Below is a figure demonstrating how Random Forest improves upon a single Decision Tree for classifying cats and dogs based on synthetic ear length and weight measurements.</p>
<figure class="align-center" id="id8">
<a class="reference internal image-reference" href="../_images/4-random-forest-example.png"><img alt="../_images/4-random-forest-example.png" src="../_images/4-random-forest-example.png" style="width: 640px;" />
</a>
<figcaption>
<p><span class="caption-text">Top row shows the classification boundaries for both models. On the left, a single Decision Tree creates rigid, rectangular decision regions that precisely follow axis-aligned splits in the training data. While this achieves a good separation of the training samples, the jagged boundaries suggest potential overfitting to noise. In contrast, the Random Forest (right) produces smoother, more nuanced decision boundaries through majority voting across 100 trees. The blended purple transition zones represent areas where individual trees disagree, demonstrating how the ensemble averages out erratic predictions from any single tree. Bottom row reveals why Random Forests are more robust by examining three constituent trees. Tree #1 prioritizes ear length for its initial split, Tree #2 begins with weight, and Tree #3 uses a completely different weight threshold.</span><a class="headerlink" href="#id8" title="Link to this image"></a></p>
</figcaption>
</figure>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="n">rf_model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">rf_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred_rf</span> <span class="o">=</span> <span class="n">rf_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="n">score_rf</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_rf</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy for Random Forest:&quot;</span><span class="p">,</span> <span class="n">score_rf</span> <span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_rf</span><span class="p">))</span>

<span class="n">cm_rf</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_rf</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm_rf</span><span class="p">,</span> <span class="s2">&quot;Confusion Matrix using Random Forest algorithm&quot;</span><span class="p">,</span> <span class="s2">&quot;confusion-matrix-rf.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/4-confusion-matrix-rf.png"><img alt="../_images/4-confusion-matrix-rf.png" src="../_images/4-confusion-matrix-rf.png" style="width: 420px;" />
</a>
</figure>
<p>In addition to the confusion matrix, feature importance in a Random Forest (and also in Decision Tree) model provides valuable insight into which input features contribute most to the model’s predictions. Random Forest calculates feature importance by evaluating how much each feature decreases impurity – such as Gini impurity or entropy – when it is used to split the data across all decision trees in the forest. The higher the total impurity reduction attributed to a feature, the more important it is considered. These importance scores are then normalized to provide a relative ranking, helping identify which features are most influential in determining the output class. This information is especially useful for interpreting model behavior, selecting meaningful features, and understanding the underlying structure of the data.</p>
<p>Below is the code example for plotting the feature importance using a Random Forest algorithm to classify penguins into three categories.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">importances</span> <span class="o">=</span> <span class="n">rf_clf</span><span class="o">.</span><span class="n">feature_importances_</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">barh</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">importances</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:orange&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Feature Importance&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Features&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Random Forest Feature Importance&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-center" id="id9">
<a class="reference internal image-reference" href="../_images/4-random-forest-feature-importrance.png"><img alt="../_images/4-random-forest-feature-importrance.png" src="../_images/4-random-forest-feature-importrance.png" style="width: 512px;" />
</a>
<figcaption>
<p><span class="caption-text">Illustration of feature importance for penguin classification. Features with longer bars indicate greater influence in the classification decision, meaning the Random Forest relies more heavily on these measurements to correctly identify species.</span><a class="headerlink" href="#id9" title="Link to this image"></a></p>
</figcaption>
</figure>
</section>
<section id="gradient-boosting">
<h3>Gradient Boosting<a class="headerlink" href="#gradient-boosting" title="Link to this heading"></a></h3>
<p>We have trained the model using Decision Tree classifier, which offers an intuitive starting point for classifying penguin species based on their physical measurements (flipper length, body mass, <em>etc.</em>). This classifier is sensitive to small fluctuations in dataset, which often leads to overfitting, especially when the tree is deep.</p>
<p>To overcome the limitations of a single decision tree, we turned to Random Forest, which is an ensemble method that constructs multiple decision trees on different random subsets of the data and features. By averaging the predictions from each tree (in classification, taking a majority vote), random forests reduce overfitting and improve generalization. This approach balances model complexity with performance, and it offers a reliable estimate of feature importance, helping us understand which physical attributes are most influential in distinguishing penguin species.</p>
<p>While random forests offer robustness and improved accuracy over individual trees, we can push performance further by using <strong>Gradient Boosting</strong>. Gradient Boosting is also an ensemble learning technique that builds a strong classifier by combining many weak learners – typically shallow decision trees – in a sequential manner. Unlike Random Forest, which grows multiple trees independently and in parallel using random subsets of the data. Gradient Boosting constructs trees one at a time, where each new tree is trained to correct the errors made by its predecessors.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/4-random-forest-vs-gradient-boosting.png"><img alt="../_images/4-random-forest-vs-gradient-boosting.png" src="../_images/4-random-forest-vs-gradient-boosting.png" style="width: 512px;" />
</a>
</figure>
<p>In this code example below, we apply Gradient Boosting algorithm to classify penguin species. We use <code class="docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code> from scikit-learn due to its simplicity and strong baseline performance.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>

<span class="n">gb_model</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                                    <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="n">gb__model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred_gb</span> <span class="o">=</span> <span class="n">gb__model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="n">score_gb</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_gb</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy for Gradient Boosting:&quot;</span><span class="p">,</span> <span class="n">score_gb</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_gb</span><span class="p">))</span>

<span class="n">cm_gb</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_gb</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm_gb</span><span class="p">,</span> <span class="s2">&quot;Confusion Matrix using Gradient Boosting algorithm&quot;</span><span class="p">,</span> <span class="s2">&quot;confusion-matrix-gb.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/4-confusion-matrix-gb.png"><img alt="../_images/4-confusion-matrix-gb.png" src="../_images/4-confusion-matrix-gb.png" style="width: 420px;" />
</a>
</figure>
<p>This progression – from a single tree’s simplicity to random forests’ robustness and finally to gradient boosting’s precision – mirrors the evolution of <strong>tree-based methods</strong> in modern ML. While random forests remain excellent for baseline performance, Gradient Boosting often achieves state-of-the-art results for structured data like ecological measurements, provided careful tuning of the learning rate and tree depth.</p>
</section>
<section id="multi-layer-perceptron-scikit-learn">
<h3>Multi-Layer Perceptron (Scikit-Learn)<a class="headerlink" href="#multi-layer-perceptron-scikit-learn" title="Link to this heading"></a></h3>
<p>A <strong>Multilayer Perceptron</strong> (MLP) is a type of artificial neural network composed of multiple layers of interconnected perceptron, or neurons, that are designed to mimic the behavior of the human brain.</p>
<p>Each neuron (below figure)</p>
<ul>
<li><p>has one or more inputs (<cite>x_1</cite>, <cite>x_2</cite>, …), <em>e.g.</em>, input data expressed as floating point numbers</p></li>
<li><p>most of the time, each neuron conducts 3 main operations:</p>
<blockquote>
<div><ul class="simple">
<li><p>take the weighted sum of the inputs where (<cite>w_1</cite>, <cite>w_2</cite>, …) indicate weights</p></li>
<li><p>add an extra constant weight (<em>i.e.</em> a bias term) to this weighted sum</p></li>
<li><p>apply an activation function</p></li>
</ul>
</div></blockquote>
</li>
<li><p>return one output value</p></li>
<li><p>one example equation to calculate the output for a neuron is <cite>output = Activation(sum_i (x_i * w_i) + bias)</cite>.</p></li>
</ul>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/4-neuron-activation-function.png"><img alt="../_images/4-neuron-activation-function.png" src="../_images/4-neuron-activation-function.png" style="width: 512px;" />
</a>
</figure>
<p>An <strong>activation function</strong> is a mathematical transformation, and it converts the weighted sum of the inputs to the output signal of the neuron (perceptron). It introduces non-linearity to the network, enabling it to learn complex patterns and make decisions based on the weighted sum of inputs.</p>
<p>Below are representative activation functions commonly used in neural networks and DL models. Each function serves the crucial role of introducing non-linearities that enable neural networks to learn complex patterns and relationships in data.</p>
<ul class="simple">
<li><p>The <strong>sigmoid</strong> function, with its characteristic S-shaped curve, maps inputs to a smooth 0-1 range, making it historically popular for binary classification tasks.</p></li>
<li><p>The hyperbolic tangent (<strong>tanh</strong>) function, similar to sigmoid but ranging between -1 and 1, often demonstrates stronger gradients during training.</p></li>
<li><p>The <strong>Rectified Linear Unit</strong> (ReLU), which outputs zero for negative inputs and the identity for positive inputs, has become the default choice for many architectures due to its computational efficiency and effectiveness at mitigating the vanishing gradient problem.</p></li>
<li><p>The <strong>linear</strong> activation function (identity function) serves as an important reference point, demonstrating what network behavior would look like without any non-linear transformation.</p></li>
</ul>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/4-activation-function.png"><img alt="../_images/4-activation-function.png" src="../_images/4-activation-function.png" style="width: 640px;" />
</a>
</figure>
<p>A single neuron (perceptron), while capable of learning simple patterns, is limited in its ability to model complex relationships. By combining multiple neurons into layers and connecting them in a network, we create a powerful computational framework capable of approximating highly non-linear functions. In a MLP, neurons are organized into an input layer, one or more hidden layers, and an output layer.</p>
<p>The image below shows an example of a three-layer perceptron network having 3, 4, and 2 neurons in input, hidden and output layers.</p>
<ul class="simple">
<li><p>The input layer receives raw data, such as pixel values or measurements, and passes them to hidden layers.</p></li>
<li><p>The hidden layer contains multiple neurons that process the information and progressively extract higher-level features. Each neuron in a hidden layer is connected to neurons in adjacent layers, forming a dense web of weighted connections.</p></li>
<li><p>Finally, the output layer produces the network’s predictions, whether it’s a classification, regression output, or some other task.</p></li>
</ul>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/4-mlp-network.png"><img alt="../_images/4-mlp-network.png" src="../_images/4-mlp-network.png" style="width: 512px;" />
</a>
</figure>
<p>Here we build a three-layer perceptron for the penguins classification task using the <code class="docutils literal notranslate"><span class="pre">MLPClassifier</span></code> from <code class="docutils literal notranslate"><span class="pre">sklearn.neural_network</span></code>, which provides built-in functionality for training using backpropagation and gradient descent.</p>
<ul class="simple">
<li><p>this model is configured with an input layer matching the number of features (here we have four features for each penguin), a hidden layer with a specified number of neurons (<em>e.g.</em>, 16) to capture non-linear relationships, and an output layer with three nodes corresponding to penguins classes, using a <code class="docutils literal notranslate"><span class="pre">relu</span></code> activation function for the hidden layer neurons.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">adam</span></code> is the optimization algorithm used to update weight parameters</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">alpha</span></code> is the L2 regularization term (penalty). Setting this to 0 disables regularization, meaning the model won’t penalize large weights. This may lead to overfitting if the dataset is noisy or small.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">batch_size</span></code> is the number of samples per mini-batch during training. Smaller batch sizes lead to more frequent weight updates, which can result in more fine-grained learning but may increase noise and training time.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> specifies the learning rate schedule. “constant” means the learning rate remains fixed throughout training. Other options like “invscaling” or “adaptive” would change the learning rate during training.</p></li>
<li><p>With a constant learning rate, the <code class="docutils literal notranslate"><span class="pre">learning_rate_init=0.001</span></code> is used throughout training. A smaller value means slower learning, which may require more iterations but offers more stability.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_iter</span></code> specifies the maximum number of training iterations (epochs).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">random_state=123</span></code> controls the random number generation for weight initialization and data shuffling, ensuring reproducible results.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_iter_no_change=10</span></code> indicates that if validation score does not improve for 10 consecutive iterations, training will stop early. This is a form of early stopping to prevent overfitting or unnecessary computation.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.neural_network</span><span class="w"> </span><span class="kn">import</span> <span class="n">MLPClassifier</span>

<span class="n">mlp_model</span> <span class="o">=</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
                  <span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span>
                  <span class="n">learning_rate_init</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                  <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span> <span class="n">n_iter_no_change</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">mlp_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
<p>After fitting the model to the training data, we evaluate its accuracy on the test set, computing and then plotting the confusion matrix.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred_mlp</span> <span class="o">=</span> <span class="n">mlp_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="n">score_mlp</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_mlp</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy for MultiLayter Perceptron:&quot;</span><span class="p">,</span> <span class="n">score_mlp</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_mlp</span><span class="p">))</span>

<span class="n">cm_mlp</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_mlp</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm_mlp</span><span class="p">,</span> <span class="s2">&quot;Confusion Matrix using Multi-Layer Perceptron algorithm&quot;</span><span class="p">,</span> <span class="s2">&quot;confusion-matrix-mlp.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/4-confusion-matrix-mlp.png"><img alt="../_images/4-confusion-matrix-mlp.png" src="../_images/4-confusion-matrix-mlp.png" style="width: 420px;" />
</a>
</figure>
</section>
<section id="deep-neural-networks-keras">
<h3>Deep Neural Networks (Keras)<a class="headerlink" href="#deep-neural-networks-keras" title="Link to this heading"></a></h3>
<p>The MLP represents a foundational architecture in neural networks, consisting of an input layer, one or more hidden layers, and an output layer. While MLPs excel at learning complex patterns from tabular data, their shallow depth (typically 1-2 hidden layers) limits their ability to handle very high-dimensional or abstract data such as raw images, audio, or text.</p>
<p>To address these limitations, deep neural networks (DNNs) extend the MLP framework by incorporating multiple hidden layers. These additional layers allow the model to learn highly abstract features through deep hierarchical representations: early layers might capture basic features (like edges or shapes), while deeper layers recognize complex objects or semantic patterns. This depth enables DNNs to outperform traditional MLPs in complex tasks requiring high-level feature extraction, such as computer vision and natural language processing.</p>
<p>DNNs have specialized architectures designed to handle different types of data (<em>e.g.</em>, spatial, temporal, and sequential data) and tasks more effectively.</p>
<ul class="simple">
<li><p>a standard feedforward deep neural network consists of stacked fully connected layers</p></li>
<li><p><strong>convolutional neural networks</strong> (CNNs) are particularly well-suited for image data. They use convolutional layers to automatically extract local features like edges, textures, and shapes, significantly reducing the number of parameters and improving generalization on visual tasks.</p></li>
<li><p><strong>recurrent neural network</strong> (RNN) is designed for sequential data such as time series, speech, or natural language. RNNs include loops that allow information to persist across time steps, enabling the model to learn dependencies over sequences. More advanced versions, like Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs), address the limitations of basic RNNs by managing long-term dependencies more effectively.</p></li>
<li><p>In addition to CNNs and RNNs, the <strong>Transformer</strong> architecture has emerged as the state-of-the-art in many language and vision tasks. Transformers rely entirely on attention mechanisms rather than recurrence or convolutions, enabling them to model global relationships in data more efficiently. This flexibility has made them the foundation of powerful models like BERT, GPT, and Vision Transformers (ViTs). These specialized deep learning architectures illustrate how tailoring the network design to the structure of the data can lead to significant performance gains and more efficient learning.</p></li>
</ul>
<p>Here we use the Keras package to construct a small DNN and apply it to the penguins classification task, demonstrating how even a compact architecture can effectively distinguish between penguin species (Adelie, Chinstrap, and Gentoo).</p>
<p>Since Keras is part of the TensorFlow framework, we need to install TensorFlow if it hasn’t been installed already. In a Jupyter notebook, we can run the command <code class="docutils literal notranslate"><span class="pre">!pip</span> <span class="pre">install</span> <span class="pre">tensorflow</span></code>. After installation, it’s recommended to comment out the installation command and restart the kernel to ensure the environment is properly updated before running the rest of the notebook.</p>
<p>In this example, we do not use the categorical features “island” and “sex”, so we remove them from both the training and testing datasets. We then encode the target label “species” using the <code class="docutils literal notranslate"><span class="pre">pd.get_dummies</span></code> method. After that, we split the data into training and testing sets and standardize the feature values to ensure consistent scaling for model training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow</span><span class="w"> </span><span class="kn">import</span> <span class="n">keras</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">penguins_classification</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;species&#39;</span><span class="p">,</span><span class="s1">&#39;island&#39;</span><span class="p">,</span> <span class="s1">&#39;sex&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">penguins_classification</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;int&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">penguins_classification</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
<span class="n">y</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Adelie&#39;</span><span class="p">,</span> <span class="s1">&#39;Chinstrap&#39;</span><span class="p">,</span> <span class="s1">&#39;Gentoo&#39;</span><span class="p">]</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of examples for training is </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span><span class="si">}</span><span class="s2"> and test is </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
<p>When building a neural network model with Keras, there are two common approaches: using the <code class="docutils literal notranslate"><span class="pre">Sequential()</span></code> API in a step-by-step manner, or defining all layers at once within the <code class="docutils literal notranslate"><span class="pre">Sequential()</span></code> constructor.</p>
<p>In the first approach, we start by creating an empty model using <code class="docutils literal notranslate"><span class="pre">keras.Sequential()</span></code>, which initializes a sequential container for stacking layers in a linear fashion. Then we define each layer separately using the <code class="docutils literal notranslate"><span class="pre">Dense</span></code> class, specifying the number of neurons and activation functions for each layer, and finally stack all layers to a trainable model using <code class="docutils literal notranslate"><span class="pre">keras.Model()</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tensorflow.keras.layers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Dropout</span>

<span class="n">dnn_model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>

<span class="n">input_layer</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],))</span> <span class="c1"># 4 input features</span>

<span class="n">hidden_layer1</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">input_layer</span><span class="p">)</span>
<span class="n">hidden_layer1</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)(</span><span class="n">hidden_layer1</span><span class="p">)</span>

<span class="n">hidden_layer2</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">hidden_layer1</span><span class="p">)</span>
<span class="c1">#hidden_layer2 = Dropout(0.0)(hidden_layer2)</span>

<span class="n">hidden_layer3</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">hidden_layer2</span><span class="p">)</span>

<span class="n">output_layer</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)(</span><span class="n">hidden_layer3</span><span class="p">)</span> <span class="c1"># 3 classes</span>

<span class="n">dnn_model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">input_layer</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">output_layer</span><span class="p">)</span>
</pre></div>
</div>
<p>Alternatively, we can streamline the process by defining all layers inside the <code class="docutils literal notranslate"><span class="pre">Sequential()</span></code> constructor. This approach creates the model and its architecture in a single, compact step, improving readability and reducing boilerplate code. It’s convenient for simple feedforward networks where the layer order is linear and straightforward.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dnn_model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
   <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],)),</span> <span class="c1"># input: 4 input features</span>

   <span class="n">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
   <span class="c1"># combine two lines together &quot;Dense(32, activation=&#39;relu&#39;, input_shape=(X_train_scaled.shape[1],)),&quot;</span>
   <span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">),</span>

   <span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
   <span class="c1"># Dropout(0.0),</span>

   <span class="n">Dense</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>

   <span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)</span> <span class="c1"># output: 3 classes</span>
<span class="p">])</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">keras.layers.Dropout()</span></code> is a regularization layer in Keras used to reduce overfitting in neural networks by randomly setting a fraction of input units to zero during training. <code class="docutils literal notranslate"><span class="pre">Dropout(0.2)</span></code> means 20% of the outputs of a specific layer will be set to zero randomly.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/4-dnn-network-dropout.png"><img alt="../_images/4-dnn-network-dropout.png" src="../_images/4-dnn-network-dropout.png" style="width: 512px;" />
</a>
</figure>
<p>We use <code class="docutils literal notranslate"><span class="pre">dnn_model.summary()</span></code> to print a concise summary of a neural network’s architecture. It provides an overview of the model’s layers, their output shapes, and the number of trainable parameters, helping you debug and understand the network’s structure.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/4-dnn-summary.png"><img alt="../_images/4-dnn-summary.png" src="../_images/4-dnn-summary.png" style="width: 640px;" />
</a>
</figure>
<p>Now we have designed a DNN that, in theory, should be capable of learning to classify penguins. However, before training can begin, we must specify two critical components: (1) a loss function to quantify prediction errors, (2) an optimizer to adjust the model’s weights during training</p>
<ul class="simple">
<li><p>For the loss function, we select categorical cross-entropy for multi-class classification, as it penalizes incorrect probabilistic predictions. In Keras this is implemented in the <code class="docutils literal notranslate"><span class="pre">keras.losses.CategoricalCrossentropy</span></code> class. This loss function works well in combination with the <code class="docutils literal notranslate"><span class="pre">softmax</span></code> activation function we chose earlier. For more information on the available loss functions in Keras you can check the <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses">documentation</a>.</p></li>
<li><p>The optimizer determines how efficiently the model converges to a solution. Keras gives us plenty of choices all of which have their own pros and cons, but for now let us go with the widely used <code class="docutils literal notranslate"><span class="pre">Adam</span></code> (adaptive momentum estimation) optimizer. Adam has a number of parameters, but the default values work well for most problems, and therefore we use it with its default parameters.</p></li>
</ul>
<p>We use <code class="docutils literal notranslate"><span class="pre">model.compile()</span></code> to combine the determined loss function and optimier together, before starting the training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">keras.optimizers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Adam</span>

<span class="n">dnn_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">CategoricalCrossentropy</span><span class="p">())</span>
</pre></div>
</div>
<p>We are now ready to train the DNN model. Here we only set a different number of <code class="docutils literal notranslate"><span class="pre">epochs</span></code>. One training epoch means that every sample in the training data has been shown to the neural network and used to update its parameters. During training, we set <code class="docutils literal notranslate"><span class="pre">batch_size=16</span></code> to balance memory efficiency and gradient stability, while <code class="docutils literal notranslate"><span class="pre">verbose=1</span></code> enables progress bars to monitor each epoch’s loss and metrics in real-time.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">history</span> <span class="o">=</span> <span class="n">dnn_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">fit</span></code> method returns a history object that has a history attribute with the training loss and potentially other metrics per training epoch. It can be very insightful to plot the training loss to see how the training progresses. Using seaborn we can do this as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">history</span><span class="o">.</span><span class="n">epoch</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;tab:orange&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training Loss&#39;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/4-dnn-loss.png"><img alt="../_images/4-dnn-loss.png" src="../_images/4-dnn-loss.png" style="width: 512px;" />
</a>
</figure>
<p>Finally we evaluate its accuracy on the test set, computing and then plotting the confusion matrix.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># predict class probabilities</span>
<span class="n">y_pred_dnn_probs</span> <span class="o">=</span> <span class="n">dnn_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>

<span class="c1"># convert probabilities to class labels</span>
<span class="n">y_pred_dnn</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_pred_dnn_probs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">score_dnn</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred_dnn</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy for Deep Neutron Network:&quot;</span><span class="p">,</span> <span class="n">score_dnn</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred_dnn</span><span class="p">))</span>

<span class="n">cm_dnn</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred_dnn</span><span class="p">)</span>
<span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">cm_dnn</span><span class="p">,</span> <span class="s2">&quot;Confusion Matrix using DNN algorithm&quot;</span><span class="p">,</span> <span class="s2">&quot;confusion-matrix-dnn.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/4-confusion-matrix-dnn.png"><img alt="../_images/4-confusion-matrix-dnn.png" src="../_images/4-confusion-matrix-dnn.png" style="width: 420px;" />
</a>
</figure>
</section>
</section>
<section id="comparison-of-trained-models">
<h2>Comparison of Trained Models<a class="headerlink" href="#comparison-of-trained-models" title="Link to this heading"></a></h2>
<p>To evaluate the performance of different ML algorithms in classifying penguin species, we compared their accuracy scores and confusion matrices. The algorithms tested include instance-based algorithm (KNN), probability-based algorithms (Logistic Regression and Naive Bayes), SVM, tree-based methods (Decision Tree, Random Forest, and Gradient Boosting), and network-based models (Multi-Layer Perceptron and Deep Neural Networks). Each model was trained on the same training set and evaluated on a common test set, with consistent preprocessing applied across all methods.</p>
<p>For the current settings for the training:</p>
<ul class="simple">
<li><p>the Multi-Layer Perceptron algorithm achieved the highest accuracy, demonstrating their effectiveness in capturing complex patterns and feature interactions in the Penguins dataset;</p></li>
<li><p>the Naive Bayes algorithm showed slightly lower accuracy, likely due to its strong independence assumption between features, which doesn’t fully hold in the dataset;</p></li>
<li><p>the other algorithms provided moderate performance.</p></li>
</ul>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/4-scores-for-all-models.png"><img alt="../_images/4-scores-for-all-models.png" src="../_images/4-scores-for-all-models.png" style="width: 640px;" />
</a>
</figure>
<p>The confusion matrices provided consistent and deeper insights into how each model handled class-level predictions:</p>
<ul class="simple">
<li><p>the Multi-Layer Perceptron algorithm showed well-balanced performance across all three penguin species;</p></li>
<li><p>the Naive Bayes algorithm, in contrast, confused Adelie and Chinstrap penguins, which can be attributed to overlapping feature distributions between these species;</p></li>
<li><p>the other algorithms made limited number of misclassified instances (mainly between Adelie and Chinstrap).</p></li>
</ul>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/4-compare-confusion-matrices.png"><img alt="../_images/4-compare-confusion-matrices.png" src="../_images/4-compare-confusion-matrices.png" style="width: 640px;" />
</a>
</figure>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../03-scientific-data-for-ML/" class="btn btn-neutral float-left" title="Scientific Data for Machine Learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../05-supervised-ML-regression/" class="btn btn-neutral float-right" title="Supervised Learning (II): Regression" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, The contributors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>